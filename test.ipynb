{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "646c273e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'exchange'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m symbols = r.json()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Filter for exchange = \"NSE\" or \"BSE\" to get relevant symbols\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m nse_symbols = [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m symbols \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexchange\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[33m\"\u001b[39m\u001b[33mNSE\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m bse_symbols = [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m symbols \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[33m\"\u001b[39m\u001b[33mexchange\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mBSE\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'exchange'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://margincalculator.angelbroking.com/OpenAPI_File/files/OpenAPIScripMaster.json\"\n",
    "r = requests.get(url)\n",
    "symbols = r.json()\n",
    "# Filter for exchange = \"NSE\" or \"BSE\" to get relevant symbols\n",
    "nse_symbols = [s for s in symbols if s[\"exchange\"] == \"NSE\"]\n",
    "bse_symbols = [s for s in symbols if s[\"exchange\"] == \"BSE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "081374ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         symbol       date       open       high        low      close  \\\n",
      "0  ANDHRAPET.BO 2024-12-30  74.570000  75.150002  73.000000  73.559998   \n",
      "1  ANDHRAPET.BO 2024-12-31  73.500000  75.300003  73.500000  74.379997   \n",
      "2  ANDHRAPET.BO 2025-01-01  75.400002  77.000000  74.150002  76.519997   \n",
      "3  ANDHRAPET.BO 2025-01-02  77.250000  77.250000  74.510002  75.839996   \n",
      "4  ANDHRAPET.BO 2025-01-03  75.000000  76.400002  75.000000  75.440002   \n",
      "5  ANDHRAPET.BO 2025-01-06  75.989998  75.989998  72.000000  72.230003   \n",
      "6  ANDHRAPET.BO 2025-01-07  72.330002  73.400002  70.000000  71.580002   \n",
      "7  ANDHRAPET.BO 2025-01-08  71.730003  72.470001  71.099998  71.879997   \n",
      "8  ANDHRAPET.BO 2025-01-09  71.220001  74.470001  71.220001  72.059998   \n",
      "9  ANDHRAPET.BO 2025-01-10  73.500000  73.500000  70.050003  71.019997   \n",
      "\n",
      "   adj_close   volume  dividends  stock_splits  \n",
      "0  73.559998  51024.0        0.0           0.0  \n",
      "1  74.379997  37390.0        0.0           0.0  \n",
      "2  76.519997  69668.0        0.0           0.0  \n",
      "3  75.839996  53697.0        0.0           0.0  \n",
      "4  75.440002  50705.0        0.0           0.0  \n",
      "5  72.230003  82644.0        0.0           0.0  \n",
      "6  71.580002  65734.0        0.0           0.0  \n",
      "7  71.879997  73240.0        0.0           0.0  \n",
      "8  72.059998  44162.0        0.0           0.0  \n",
      "9  71.019997  54618.0        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "STOCKS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\OHCLV Data\\stocks.duckdb\"\n",
    "\n",
    "with duckdb.connect(STOCKS_DB, read_only=True) as con:\n",
    "    df = con.execute(\"SELECT * FROM yahoo_ohlcv LIMIT 10\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88deb6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name\n",
      "0  yahoo_ohlcv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "STOCKS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\OHCLV Data\\stocks.duckdb\"\n",
    "with duckdb.connect(STOCKS_DB, read_only=True) as con:\n",
    "    tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99601a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name\n",
      "0  signals\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Signals Data\\signals.duckdb\"\n",
    "\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "\n",
    "print(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75af8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          symbol       date  rsi_signal  rsi_weight  macd_signal  macd_weight  \\\n",
      "0    RITESHIN.BO 2025-06-27         0.0         1.2   -45.054924          1.5   \n",
      "1  ATHERENERG.NS 2025-06-27         0.0         1.2    94.151493          1.5   \n",
      "2    SIBARAUT.BO 2025-06-27         0.0         1.2    23.818674          1.5   \n",
      "3     SHIVAGR.BO 2025-06-27         0.0         1.2   -28.533828          1.5   \n",
      "\n",
      "   adx_signal  adx_weight  ema_crossover_signal  ema_crossover_weight  \\\n",
      "0   12.795962         1.0            -11.685119                   1.3   \n",
      "1   21.300294         1.0            100.000000                   1.3   \n",
      "2   27.078748         1.0             23.624305                   1.3   \n",
      "3   23.149416         1.0            -50.128022                   1.3   \n",
      "\n",
      "   vwap_signal  vwap_weight  ichimoku_cloud_signal  ichimoku_cloud_weight  \\\n",
      "0     2.527142          1.0                    NaN                    1.5   \n",
      "1    66.720185          1.0                    NaN                    1.5   \n",
      "2     2.483989          1.0                    NaN                    1.5   \n",
      "3   -10.978404          1.0                    NaN                    1.5   \n",
      "\n",
      "   bollinger_bands_signal  bollinger_bands_weight  atr_signal  atr_weight  \\\n",
      "0             -100.000000                     0.9   65.743882         1.0   \n",
      "1               39.211481                     0.9   66.261508         1.0   \n",
      "2               32.548568                     0.9   86.465241         1.0   \n",
      "3              -12.829017                     0.9   70.918669         1.0   \n",
      "\n",
      "   obv_signal  obv_weight  \n",
      "0   -1.982894         1.0  \n",
      "1   -6.152294         1.0  \n",
      "2    0.000000         1.0  \n",
      "3 -100.000000         1.0  \n",
      "Index(['symbol', 'date', 'rsi_signal', 'rsi_weight', 'macd_signal',\n",
      "       'macd_weight', 'adx_signal', 'adx_weight', 'ema_crossover_signal',\n",
      "       'ema_crossover_weight', 'vwap_signal', 'vwap_weight',\n",
      "       'ichimoku_cloud_signal', 'ichimoku_cloud_weight',\n",
      "       'bollinger_bands_signal', 'bollinger_bands_weight', 'atr_signal',\n",
      "       'atr_weight', 'obv_signal', 'obv_weight'],\n",
      "      dtype='object')\n",
      "Rows: 4 Columns: 20\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "SIGNALS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Signals Data\\signals.duckdb\"\n",
    "\n",
    "with duckdb.connect(SIGNALS_DB, read_only=True) as con:\n",
    "    df = con.execute(\"SELECT * FROM signals LIMIT 4\").fetchdf()\n",
    "\n",
    "print(df.head())        # See first few rows/columns\n",
    "print(df.columns)       # List of all columns\n",
    "print(f\"Rows: {len(df)} Columns: {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9fea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in DB:\n",
      "           name\n",
      "0  bse_metadata\n",
      "1   bse_symbols\n",
      "2   nse_symbols\n",
      "\n",
      "Top 4 rows from 'bse_metadata':\n",
      "   SC_CODE    SC_NAME SC_GROUP SC_TYPE     OPEN     HIGH      LOW    CLOSE  \\\n",
      "0   500002        ABB       A        Q  6033.30  6060.00  5943.70  5966.60   \n",
      "1   500003   AEGISLOG       A        Q   793.05   801.95   790.15   795.85   \n",
      "2   500008      ARE&M       A        Q   973.80   979.45   967.00   969.40   \n",
      "3   500009  AMBALALSA       X        Q    36.26    36.90    35.12    35.52   \n",
      "\n",
      "   LAST_TRD_PRICE  PREV_CLOSE  ... TOTAL_NO_OF_SHRS  FREE_FLOAT_NO_OF_SHRS  \\\n",
      "0         5966.60     6003.00  ...      211908375.0           5.229835e+07   \n",
      "1          795.85      787.40  ...      351000000.0           1.253428e+08   \n",
      "2          969.40      971.40  ...      183025364.0           1.221498e+08   \n",
      "3           35.52       35.73  ...       76633296.0           5.137941e+07   \n",
      "\n",
      "   52_W_HIGH  52_W_LOW  FACE_VALUE      EXDATE  PURPOSE  \\\n",
      "0    8941.45   4590.05         2.0        None      NaN   \n",
      "1    1035.70    610.50         1.0  25/06/2025      NaN   \n",
      "2    1774.90    805.05         1.0        None      NaN   \n",
      "3      77.70     34.11        10.0        None      NaN   \n",
      "\n",
      "                         INDUSTRY  UNNAMED:_26  MISSING_DATA  \n",
      "0                   Capital Goods          NaN           Yes  \n",
      "1     Oil, Gas & Consumable Fuels          NaN           Yes  \n",
      "2  Automobile and Auto Components          NaN           Yes  \n",
      "3                      Healthcare          NaN           Yes  \n",
      "\n",
      "[4 rows x 28 columns]\n",
      "Total rows in bse_metadata: 4500\n",
      "\n",
      "Top 4 rows from 'bse_symbols':\n",
      "   SYMBOL                               NAME MISSING_DATA\n",
      "0  500002                  ABB India Limited           No\n",
      "1  500003               AEGIS LOGISTICS LTD.           No\n",
      "2  500008          AMARA RAJA BATTERIES LTD.           No\n",
      "3  500009  AMBALAL SARABHAI ENTERPRISES LTD.           No\n",
      "Total rows in bse_symbols: 5066\n",
      "\n",
      "Top 4 rows from 'nse_symbols':\n",
      "       SYMBOL                           NAME_OF_COMPANY SERIES  \\\n",
      "0   20MICRONS                        20 Microns Limited     EQ   \n",
      "1  21STCENMGM  21st Century Management Services Limited     EQ   \n",
      "2      360ONE                       360 ONE WAM LIMITED     EQ   \n",
      "3   3IINFOLTD                       3i Infotech Limited     EQ   \n",
      "\n",
      "  DATE_OF_LISTING  PAID_UP_VALUE  MARKET_LOT   ISIN_NUMBER  FACE_VALUE  \\\n",
      "0     06-OCT-2008              5           1  INE144J01027           5   \n",
      "1     03-MAY-1995             10           1  INE253B01015          10   \n",
      "2     19-SEP-2019              1           1  INE466L01038           1   \n",
      "3     22-OCT-2021             10           1  INE748C01038          10   \n",
      "\n",
      "  MISSING_DATA  \n",
      "0           No  \n",
      "1           No  \n",
      "2           No  \n",
      "3           No  \n",
      "Total rows in nse_symbols: 2100\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# List all tables\n",
    "tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "print(\"Tables in DB:\")\n",
    "print(tables)\n",
    "\n",
    "# Fetch and display top 4 rows from each table\n",
    "for table_name in tables['name']:\n",
    "    print(f\"\\nTop 4 rows from '{table_name}':\")\n",
    "    df = con.execute(f\"SELECT * FROM {table_name} LIMIT 4\").fetchdf()\n",
    "    print(df)\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "    print(f\"Total rows in {table_name}: {total_rows}\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3024141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE symbols:\n",
      "   NSE_SYMBOL                              COMPANY_NAME\n",
      "0   20MICRONS                        20 Microns Limited\n",
      "1  21STCENMGM  21st Century Management Services Limited\n",
      "2      360ONE                       360 ONE WAM LIMITED\n",
      "3   3IINFOLTD                       3i Infotech Limited\n",
      "4     3MINDIA                          3M India Limited\n",
      "5      3PLAND                  3P Land Holdings Limited\n",
      "6      5PAISA                    5Paisa Capital Limited\n",
      "7     63MOONS             63 moons technologies limited\n",
      "8    A2ZINFRA             A2Z Infra Engineering Limited\n",
      "9     AAATECH                  AAA Technologies Limited\n",
      "\n",
      "BSE symbols:\n",
      "   BSE_SYMBOL                            COMPANY_NAME\n",
      "0         ABB                       ABB India Limited\n",
      "1    AEGISLOG                    AEGIS LOGISTICS LTD.\n",
      "2       ARE&M               AMARA RAJA BATTERIES LTD.\n",
      "3   AMBALALSA       AMBALAL SARABHAI ENTERPRISES LTD.\n",
      "4   ANDHRAPET              ANDHRA PETROCHEMICALS LTD.\n",
      "5    ANSALAPI  ANSAL PROPERTIES & INFRASTRUCTURE LTD.\n",
      "6      UTIQUE                  Utique Enterprises Ltd\n",
      "7   ARUNAHTEL                       ARUNA HOTELS LTD.\n",
      "8   BOMDYEING             BOMBAY DYEING & MFG.CO.LTD.\n",
      "9  ASIANHOTNR            Asian Hotels (North) Limited\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "# ----- NSE Mapping -----\n",
    "with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "    nse_df = con.execute(\"SELECT SYMBOL, NAME_OF_COMPANY FROM nse_symbols\").fetchdf()\n",
    "    nse_df['SYMBOL'] = nse_df['SYMBOL'].str.upper()\n",
    "    nse_name_df = nse_df.rename(columns={\"SYMBOL\": \"NSE_SYMBOL\", \"NAME_OF_COMPANY\": \"COMPANY_NAME\"})\n",
    "\n",
    "print(\"NSE symbols:\")\n",
    "print(nse_name_df.head(10))\n",
    "\n",
    "# ----- BSE Mapping -----\n",
    "bse_map = {}\n",
    "\n",
    "with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "    # 1. Join SC_NAME from meta with NAME from symbols\n",
    "    bse_df = con.execute(\"\"\"\n",
    "        SELECT meta.SC_NAME, sym.NAME\n",
    "        FROM bse_metadata AS meta\n",
    "        JOIN bse_symbols AS sym\n",
    "        ON CAST(meta.SC_CODE AS VARCHAR) = CAST(sym.SYMBOL AS VARCHAR)\n",
    "        WHERE meta.SC_NAME IS NOT NULL\n",
    "    \"\"\").fetchdf()\n",
    "    for _, row in bse_df.iterrows():\n",
    "        sc_name = str(row['SC_NAME']).upper()\n",
    "        name = row['NAME'] if pd.notnull(row['NAME']) else \"\"\n",
    "        bse_map[sc_name] = name\n",
    "\n",
    "    # 2. Direct fallback: SYMBOL+\".BO\" -> NAME\n",
    "    direct_df = con.execute(\"SELECT SYMBOL, NAME FROM bse_symbols\").fetchdf()\n",
    "    for _, row in direct_df.iterrows():\n",
    "        symbol = str(row['SYMBOL']).upper()\n",
    "        name = row['NAME'] if pd.notnull(row['NAME']) else \"\"\n",
    "        bo_symbol = symbol + \".BO\"\n",
    "        if bo_symbol not in bse_map or bse_map[bo_symbol] == \"\":\n",
    "            bse_map[bo_symbol] = name\n",
    "\n",
    "bse_name_df = pd.DataFrame(list(bse_map.items()), columns=['BSE_SYMBOL', 'COMPANY_NAME'])\n",
    "\n",
    "print(\"\\nBSE symbols:\")\n",
    "print(bse_name_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from indicators import *\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "# ===== PATHS =====\n",
    "ROOT = Path(r\"C:/Users/jallu/OneDrive/pgp/Python/Stock predictor/Rubik_view\")\n",
    "EXCEL_FILE = ROOT / \"rubikview.xlsm\"\n",
    "INDICATOR_SHEET = \"Indicators\"\n",
    "OHLCV_DB = ROOT / \"Data\" / \"OHCLV Data\" / \"stocks.duckdb\"\n",
    "SIGNALS_DB = ROOT / \"Data\" / \"Signals Data\" / \"signals.duckdb\"\n",
    "DASH_SHEET = \"Update Dash board\"\n",
    "PROGRESS_CELL = \"K18\"\n",
    "\n",
    "progress_lock = threading.Lock()\n",
    "\n",
    "def update_excel_summary(done, total, messages, dash_sheet):\n",
    "    success_count = sum(1 for m in messages if \"processed\" in m)\n",
    "    fail_count = sum(1 for m in messages if \"error\" in m)\n",
    "    skipped_count = sum(1 for m in messages if \"skipped\" in m)\n",
    "    uptodate_count = sum(1 for m in messages if \"up-to-date\" in m)\n",
    "    processed_symbols = success_count + fail_count + skipped_count + uptodate_count\n",
    "    progress_pct = (processed_symbols / total * 100) if total else 0\n",
    "    progress_count_str = f\"{done}/{total}\"\n",
    "    headers = [\n",
    "        \"Table\", \"Progress\", \"Progress Count\", \"Success Count\", \"Failed\", \"Skipped\",\n",
    "        \"Uptodate\", \"Processed\", \"Update Date\"\n",
    "    ]\n",
    "    row = [\n",
    "        f\"{SIGNALS_DB.name}\", f\"{progress_pct:.2f}%\", progress_count_str, success_count, fail_count, skipped_count,\n",
    "        uptodate_count, processed_symbols, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ]\n",
    "    dash_sheet.range(\"H18\").value = headers\n",
    "    dash_sheet.range(\"H19\").value = row\n",
    "\n",
    "def update_excel_progress(done, total, messages):\n",
    "    try:\n",
    "        with progress_lock:\n",
    "            book = xw.Book(str(EXCEL_FILE))\n",
    "            sht = book.sheets[DASH_SHEET]\n",
    "            # Compute current success count\n",
    "            success_count = sum(1 for m in messages if \"processed\" in m)\n",
    "            # Only update K18 if there is at least one successful result, otherwise leave as-is\n",
    "            if success_count > 0:\n",
    "                sht.range(PROGRESS_CELL).value = success_count\n",
    "            # Update the summary table\n",
    "            update_excel_summary(done, total, messages, sht)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    # ===== Optionally clear dashboard summary area at start (but DO NOT clear K18) =====\n",
    "    try:\n",
    "        book = xw.Book(str(EXCEL_FILE))\n",
    "        sht = book.sheets[DASH_SHEET]\n",
    "        # DO NOT clear K18 at start; this prevents 0/blank flashes\n",
    "        sht.range(\"H18:P25\").value = \"\"   # clear a bit extra just in case\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ===== READ INDICATOR CONFIG FROM EXCEL =====\n",
    "    try:\n",
    "        sht = xw.Book(str(EXCEL_FILE)).sheets[INDICATOR_SHEET]\n",
    "        ind_df = sht.range(\"A1\").options(pd.DataFrame, header=1, index=False, expand='table').value\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not read Excel config: {e}\")\n",
    "        raise SystemExit\n",
    "    active_inds = ind_df[ind_df['Active'].str.upper() == 'Y'].copy()\n",
    "    if active_inds.empty:\n",
    "        print(\"No active indicators! Exiting.\")\n",
    "        exit(1)\n",
    "    print(f\"Active indicators: {len(active_inds)}\")\n",
    "\n",
    "    # ===== GET SYMBOLS FROM OHLCV DB =====\n",
    "    with duckdb.connect(str(OHLCV_DB), read_only=True) as con:\n",
    "        symbols = con.execute(\"SELECT DISTINCT symbol FROM yahoo_ohlcv\").fetchdf()['symbol'].tolist()\n",
    "    print(f\"Symbols: {len(symbols)}\")\n",
    "\n",
    "    # ===== PREP SIGNALS DB =====\n",
    "    with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS signals (\n",
    "                symbol VARCHAR,\n",
    "                date DATE\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # ===== INDICATOR CALCULATION MAP =====\n",
    "    def calculate_indicator(name, df, p1, p2, p3):\n",
    "        if name == \"RSI\":\n",
    "            return calculate_rsi(df['Close'], int(p1))\n",
    "        elif name == \"MFI\":\n",
    "            return calculate_mfi(df['High'], df['Low'], df['Close'], df['Volume'], int(p1))\n",
    "        elif name == \"CCI\":\n",
    "            return calculate_cci(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"StochRSI\":\n",
    "            return calculate_stochrsi(df['Close'], int(p1), int(p2), int(p3))\n",
    "        elif name == \"ROC\":\n",
    "            return calculate_roc(df['Close'], int(p1))\n",
    "        elif name == \"MACD\":\n",
    "            return calculate_macd(df['Close'], int(p1), int(p2), int(p3))\n",
    "        elif name == \"EMA Crossover\":\n",
    "            return calculate_ema_crossover(df['Close'], int(p1), int(p2))\n",
    "        elif name == \"SMA Crossover\":\n",
    "            return calculate_sma_crossover(df['Close'], int(p1), int(p2))\n",
    "        elif name == \"ATR\":\n",
    "            return calculate_atr(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"Williams %R\":\n",
    "            return calculate_williams_r(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"ADX\":\n",
    "            return calculate_adx(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"VWAP\":\n",
    "            return calculate_vwap(df)\n",
    "        elif name == \"SuperTrend\":\n",
    "            return calculate_supertrend(df, int(p1), float(p2))\n",
    "        elif name == \"Parabolic SAR\":\n",
    "            return calculate_parabolic_sar(df['High'], df['Low'], float(p1), float(p2), float(p3))\n",
    "        elif name == \"Ichimoku\":\n",
    "            return calculate_ichimoku(df, int(p1), int(p2), int(p3))\n",
    "        elif name == \"Bollinger Bands\":\n",
    "            return calculate_bollinger(df['Close'], int(p1), float(p2))\n",
    "        elif name == \"Donchian Channel\":\n",
    "            return calculate_donchian(df['High'], df['Low'], int(p1))\n",
    "        elif name == \"Keltner Channel\":\n",
    "            return calculate_keltner(df, int(p1))\n",
    "        elif name == \"OBV\":\n",
    "            return calculate_obv(df['Close'], df['Volume'])\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # ===== MAIN PER-SYMBOL WORKER =====\n",
    "    def process_symbol(sym):\n",
    "        try:\n",
    "            with duckdb.connect(str(OHLCV_DB), read_only=True) as con:\n",
    "                df = con.execute(\n",
    "                    \"SELECT date, open, high, low, close, adj_close, volume FROM yahoo_ohlcv WHERE symbol=? ORDER BY date\",\n",
    "                    (sym,)\n",
    "                ).fetchdf()\n",
    "            if df.empty:\n",
    "                return None, f\"{sym} | skipped (no data)\"\n",
    "            last_date = df['date'].iloc[-1]\n",
    "            with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "                already = con.execute(\"SELECT COUNT(*) FROM signals WHERE symbol=? AND date=?\", (sym, last_date)).fetchone()[0]\n",
    "            if already:\n",
    "                return None, f\"{sym} | up-to-date\"\n",
    "            df = df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'})\n",
    "            row = {'symbol': sym, 'date': last_date}\n",
    "            for _, ind in active_inds.iterrows():\n",
    "                name = ind['Indicator_Name']\n",
    "                p1 = ind['Parameter_1'] if not pd.isnull(ind['Parameter_1']) else None\n",
    "                p2 = ind['Parameter_2'] if not pd.isnull(ind['Parameter_2']) else None\n",
    "                p3 = ind['Parameter_3'] if not pd.isnull(ind['Parameter_3']) else None\n",
    "                mweight = float(ind['Manual_Weight']) if not pd.isnull(ind['Manual_Weight']) else 1.0\n",
    "                use_ai  = str(ind['Use_AI_Weight']).upper() == \"Y\"\n",
    "                aiweight = float(ind['AI_Latest_Weight']) if not pd.isnull(ind['AI_Latest_Weight']) else mweight\n",
    "                weight = aiweight if use_ai else mweight\n",
    "                try:\n",
    "                    signal = calculate_indicator(name, df, p1, p2, p3)\n",
    "                except Exception as ee:\n",
    "                    signal = np.nan\n",
    "                icol = name.lower().replace(\" \", \"_\").replace(\"%\", \"pct\")\n",
    "                row[f\"{icol}_signal\"] = signal   # store as float, not int!\n",
    "                row[f\"{icol}_weight\"] = weight\n",
    "            return row, f\"{sym} | processed\"\n",
    "        except Exception as e:\n",
    "            return None, f\"{sym} | error: {str(e)}\"\n",
    "\n",
    "    # ===== FAST PARALLEL EXECUTION =====\n",
    "    results = []\n",
    "    messages = []\n",
    "    N_THREADS = 20\n",
    "    UPDATE_FREQ = 10  # update summary in Excel every 10 symbols\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_THREADS) as executor:\n",
    "        futures = {executor.submit(process_symbol, sym): sym for sym in symbols}\n",
    "        done = 0\n",
    "        for fut in as_completed(futures):\n",
    "            row, msg = fut.result()\n",
    "            done += 1\n",
    "            print(f\"[{done}/{len(symbols)}] {msg}\")\n",
    "            if row is not None:\n",
    "                results.append(row)\n",
    "            messages.append(msg)\n",
    "            if done % UPDATE_FREQ == 0 or done == len(symbols):\n",
    "                update_excel_progress(done, len(symbols), messages)\n",
    "\n",
    "    print(\"âœ… Done.\")\n",
    "    update_excel_progress(len(symbols), len(symbols), messages)   # Final update\n",
    "\n",
    "    # ===== FORCE EXCEL RECALC (optional, helps show correct numbers immediately) =====\n",
    "    try:\n",
    "        book = xw.Book(str(EXCEL_FILE))\n",
    "        sht = book.sheets[DASH_SHEET]\n",
    "        sht.api.Calculate()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ===== INSERT TO SIGNALS DB =====\n",
    "    if results:\n",
    "        df_signals = pd.DataFrame(results)\n",
    "        with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "            dbcols = [c[1] for c in con.execute(\"PRAGMA table_info('signals')\").fetchall()]\n",
    "            for c in df_signals.columns:\n",
    "                if c not in dbcols:\n",
    "                    if c.endswith(\"_weight\"):\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} DOUBLE\")\n",
    "                    elif c.endswith(\"_signal\"):\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} DOUBLE\")\n",
    "                    else:\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} VARCHAR\")\n",
    "            con.register(\"batch\", df_signals)\n",
    "            con.execute(\"INSERT INTO signals SELECT * FROM batch\")\n",
    "            con.unregister(\"batch\")\n",
    "        print(\"ðŸŽ‰ All signals saved to signals.duckdb.\")\n",
    "    else:\n",
    "        print(\"âš  No new signals to insert.\")\n",
    "\n",
    "# ===== If running directly, call main =====\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105579b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import xlwings as xw\n",
    "\n",
    "# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SCRIPT_DIR   = Path(__file__).parent\n",
    "DB_PATH      = SCRIPT_DIR / \"stocks.duckdb\"\n",
    "START_DATE   = date(2019, 1, 1)\n",
    "YESTERDAY    = date.today() - timedelta(days=1)\n",
    "MAX_WORKERS  = 20\n",
    "\n",
    "EXCEL_PATH   = SCRIPT_DIR.parent / \"rubikview.xlsm\"  # adjust if needed\n",
    "EXCEL_SHEET  = \"Update Dash board\"\n",
    "EXCEL_RANGE  = \"H11\"\n",
    "\n",
    "# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def normalize(col: str) -> str:\n",
    "    return col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def get_table_columns(conn):\n",
    "    rows = conn.execute(\"PRAGMA table_info('yahoo_ohlcv')\").fetchall()\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def init_db():\n",
    "    conn = duckdb.connect(str(DB_PATH))\n",
    "    conn.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS yahoo_ohlcv (\n",
    "        symbol VARCHAR,\n",
    "        date   DATE\n",
    "      );\n",
    "    \"\"\")\n",
    "    conn.close()\n",
    "\n",
    "def load_symbols():\n",
    "    SYMBOLS_DB = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\")\n",
    "    conn = duckdb.connect(str(SYMBOLS_DB))\n",
    "\n",
    "    # NSE: Symbol with .NS\n",
    "    nse = conn.execute(\"SELECT DISTINCT symbol FROM nse_symbols WHERE symbol IS NOT NULL\").fetchdf()\n",
    "    nse_syms = nse['SYMBOL'].str.strip() + \".NS\"\n",
    "\n",
    "    # BSE: Use SC_NAME column in bse_metadata with .BO (edit to SC_CODE if you want code instead)\n",
    "    bse = conn.execute(\"SELECT DISTINCT SC_NAME FROM bse_metadata WHERE SC_NAME IS NOT NULL\").fetchdf()\n",
    "    bse_syms = bse['SC_NAME'].str.strip() + \".BO\"\n",
    "\n",
    "    all_syms = pd.concat([nse_syms, bse_syms]).dropna().unique().tolist()\n",
    "    conn.close()\n",
    "    return all_syms\n",
    "\n",
    "def insert_dynamic(conn, df: pd.DataFrame):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    df = df.rename(columns=normalize)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['symbol'] = df['symbol']\n",
    "    existing = get_table_columns(conn)\n",
    "    for col in df.columns:\n",
    "        if col not in existing:\n",
    "            conn.execute(f\"ALTER TABLE yahoo_ohlcv ADD COLUMN {col} DOUBLE;\")\n",
    "            existing.append(col)\n",
    "    to_ins = df[[c for c in existing if c in df.columns]]\n",
    "    conn.register(\"new_data\", to_ins)\n",
    "    conn.execute(\"INSERT INTO yahoo_ohlcv SELECT * FROM new_data;\")\n",
    "    conn.unregister(\"new_data\")\n",
    "\n",
    "def fetch_and_insert(symbol):\n",
    "    conn_r = duckdb.connect(str(DB_PATH))\n",
    "    last = conn_r.execute(\n",
    "        \"SELECT MAX(date) FROM yahoo_ohlcv WHERE symbol=?\", (symbol,)\n",
    "    ).fetchone()[0]\n",
    "    conn_r.close()\n",
    "\n",
    "    fetch_start = (pd.to_datetime(last).date() + timedelta(days=1)) if last else START_DATE\n",
    "    if fetch_start > YESTERDAY:\n",
    "        return symbol, 0, None, None, \"uptodate\"\n",
    "\n",
    "    df = yf.Ticker(symbol).history(\n",
    "        start=fetch_start,\n",
    "        end=YESTERDAY + timedelta(days=1),\n",
    "        auto_adjust=False,\n",
    "        actions=True\n",
    "    )\n",
    "    if df.empty:\n",
    "        return symbol, 0, None, None, \"skipped\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    conn_w = duckdb.connect(str(DB_PATH))\n",
    "    insert_dynamic(conn_w, df)\n",
    "    conn_w.close()\n",
    "\n",
    "    first_dt = df['Date'].min().date()\n",
    "    last_dt  = df['Date'].max().date()\n",
    "    return symbol, len(df), first_dt, last_dt, \"success\"\n",
    "\n",
    "def write_progress_to_excel(done, total, progress_count, success, failed, skipped, uptodate, processed, updatedate):\n",
    "    try:\n",
    "        app = xw.apps.active if xw.apps else xw.App(visible=True)\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "        except Exception:\n",
    "            wb = xw.Book(EXCEL_PATH)\n",
    "        sht = wb.sheets[EXCEL_SHEET]\n",
    "\n",
    "        headers = [\n",
    "            \"Table\", \"Progress\", \"Progress Count\", \"Success Count\", \"Failed\", \"Skipped\",\n",
    "            \"Uptodate\", \"Processed\", \"Update Date\"\n",
    "        ]\n",
    "        row = [\n",
    "            f\"{DB_PATH.name}\",\n",
    "            f\"{(processed / total * 100):.2f}%\",\n",
    "            f\"{done}/{total}\",\n",
    "            success,\n",
    "            failed,\n",
    "            skipped,\n",
    "            uptodate,\n",
    "            processed,\n",
    "            updatedate\n",
    "        ]\n",
    "        sht.range(\"H11\").value = headers\n",
    "        sht.range(\"H12\").value = row\n",
    "    except Exception as e:\n",
    "        print(f\"Excel output error: {e}\")\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    init_db()\n",
    "    symbols = load_symbols()\n",
    "    total = len(symbols)\n",
    "    today_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Trackers\n",
    "    success = failed = skipped = uptodate = 0\n",
    "    processed = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "        futures = {exe.submit(fetch_and_insert, s): s for s in symbols}\n",
    "        for idx, future in enumerate(as_completed(futures), start=1):\n",
    "            sym = futures[future]\n",
    "            percent = idx / total * 100\n",
    "            try:\n",
    "                _, count, first_dt, last_dt, status_flag = future.result()\n",
    "                processed += 1\n",
    "                if status_flag == \"success\":\n",
    "                    success += 1\n",
    "                    status = f\"+{count} rows ({first_dt}â†’{last_dt})\"\n",
    "                elif status_flag == \"skipped\":\n",
    "                    skipped += 1\n",
    "                    status = \"skipped\"\n",
    "                elif status_flag == \"uptodate\":\n",
    "                    uptodate += 1\n",
    "                    status = \"up-to-date\"\n",
    "                else:\n",
    "                    status = status_flag\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                status = f\"FAILED: {e}\"\n",
    "\n",
    "            print(f\"{idx}/{total} ({percent:.1f}%): {sym} â†’ {status}\")\n",
    "\n",
    "            # --- Excel output: Only update Excel every 10th step or last row\n",
    "            if idx % 10 == 0 or idx == total:\n",
    "                write_progress_to_excel(\n",
    "                    idx, total, f\"{idx}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "                )\n",
    "\n",
    "    # Final write\n",
    "    write_progress_to_excel(\n",
    "        total, total, f\"{total}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "    )\n",
    "    print(\"\\nâœ… Ultra-fast parallel update complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For command line testing, uncomment:\n",
    "    # xw.Book(EXCEL_PATH).set_mock_caller()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a68c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import xlwings as xw\n",
    "from datetime import datetime\n",
    "\n",
    "def update_symbol_db(write_excel=True):\n",
    "    db_dir = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\"\n",
    "    db_path = os.path.join(db_dir, \"symbols.duckdb\")\n",
    "\n",
    "    today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # === Load NSE\n",
    "    nse_eq_url = \"https://archives.nseindia.com/content/equities/EQUITY_L.csv\"\n",
    "    df_nse = pd.read_csv(nse_eq_url, skipinitialspace=True)\n",
    "    df_nse.columns = df_nse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_nse['MISSING_DATA'] = df_nse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    nse_loaded_count = len(df_nse)\n",
    "\n",
    "    # === Load BSE Master\n",
    "    bse_csv_path = os.path.join(db_dir, \"Equity.csv\")\n",
    "    df_bse = pd.read_csv(bse_csv_path, skipinitialspace=True)\n",
    "    df_bse.columns = df_bse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse['MISSING_DATA'] = df_bse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bse_loaded_count = len(df_bse)\n",
    "\n",
    "    # === Load BSE Metadata\n",
    "    bhavcopy_path = os.path.join(db_dir, \"bhavcopy.csv\")\n",
    "    df_bse_meta = pd.read_csv(bhavcopy_path, skipinitialspace=True)\n",
    "    df_bse_meta.columns = df_bse_meta.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse_meta['MISSING_DATA'] = df_bse_meta.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bsemeta_loaded_count = len(df_bse_meta)\n",
    "\n",
    "    # === Write to DuckDB\n",
    "    con = duckdb.connect(db_path)\n",
    "    con.execute(\"DROP TABLE IF EXISTS nse_symbols\")\n",
    "    con.execute(\"CREATE TABLE nse_symbols AS SELECT * FROM df_nse\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS bse_symbols\")\n",
    "    con.execute(\"CREATE TABLE bse_symbols AS SELECT * FROM df_bse\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS bse_metadata\")\n",
    "    con.execute(\"CREATE TABLE bse_metadata AS SELECT * FROM df_bse_meta\")\n",
    "\n",
    "    # === Terminal output ===\n",
    "    print(\"\\nTables present in DuckDB:\")\n",
    "    tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "    print(tables)\n",
    "\n",
    "    def table_info(table, loaded):\n",
    "        written = con.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "        print(f\"\\n--- {table} ---\")\n",
    "        print(f\"Loaded: {loaded} | Written: {written} | Updated Date: {today_str}\")\n",
    "        print(con.execute(f\"SELECT * FROM {table} LIMIT 4\").fetchdf())\n",
    "        return [table, loaded, written, today_str]\n",
    "\n",
    "    summary = [\n",
    "        table_info(\"nse_symbols\", nse_loaded_count),\n",
    "        table_info(\"bse_symbols\", bse_loaded_count),\n",
    "        table_info(\"bse_metadata\", bsemeta_loaded_count)\n",
    "    ]\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # === Write to Excel if triggered from Excel ===\n",
    "    if write_excel:\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "            sht = wb.sheets[\"Update Dash board\"]\n",
    "            sht.range(\"H3\").value = [[\"Table\", \"Loaded Count\", \"Written Count\", \"Updated Date\"]] + summary\n",
    "        except Exception as e:\n",
    "            print(\"\\n[Excel output skipped: not run from Excel or sheet missing.]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For standalone Python/terminal runs, skip Excel output:\n",
    "    update_symbol_db(write_excel=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149792bf",
   "metadata": {},
   "source": [
    "### Combining NSE/BSE INDICES CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12baefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Scrip Code                         COMPANY      ISIN No.  Close Price  \\\n",
      "0      500002               ABB INDIA LIMITED  INE117A01022      6069.60   \n",
      "1      539254  Adani Energy Solutions Limited  INE931S01010       883.55   \n",
      "2      512599          ADANI ENTERPRISES LTD.  INE423A01024      2648.35   \n",
      "3      541450      Adani Green Energy Limited  INE364U01010      1015.75   \n",
      "4      532921  ADANI PORTS AND SPECIAL ECONOM  INE742F01042      1440.10   \n",
      "\n",
      "                                      IndexName  \n",
      "0  BSE 100 LargeCap TMC Indexindex_Constituents  \n",
      "1  BSE 100 LargeCap TMC Indexindex_Constituents  \n",
      "2  BSE 100 LargeCap TMC Indexindex_Constituents  \n",
      "3  BSE 100 LargeCap TMC Indexindex_Constituents  \n",
      "4  BSE 100 LargeCap TMC Indexindex_Constituents  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "folder = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\All stocks Meta Data files\\BSE Indices contituents\")\n",
    "\n",
    "dfs = []\n",
    "for file in folder.glob(\"*.csv\"):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file)\n",
    "    # Add column for the index, derived from the file name (without .csv)\n",
    "    df['IndexName'] = file.stem\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Show a preview\n",
    "print(combined.head())\n",
    "\n",
    "# Optionally, save to a master CSV\n",
    "combined.to_csv(folder / \"Indices_Combined.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "nifty_csv = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\All stocks Meta Data files\\All Nifty Indices Meta Data.csv\"\n",
    "\n",
    "# 1. Read the new All Nifty Indices CSV\n",
    "df_nifty = pd.read_csv(nifty_csv)\n",
    "df_nifty.columns = [col.strip() for col in df_nifty.columns]\n",
    "\n",
    "# 2. Write to DuckDB as new table and do everything else in the same context\n",
    "with duckdb.connect(db_path) as con:\n",
    "    # Create new table\n",
    "    con.execute(\"DROP TABLE IF EXISTS All_Nifty_Indices_Meta_Data\")\n",
    "    con.execute(\"CREATE TABLE All_Nifty_Indices_Meta_Data AS SELECT * FROM df_nifty\")\n",
    "    print(\"âœ… All_Nifty_Indices_Meta_Data table created.\")\n",
    "\n",
    "    # List all tables in DB\n",
    "    all_tables = con.execute(\"SHOW TABLES\").fetchdf()['name'].tolist()\n",
    "    print(\"\\nTables currently in DB:\", all_tables)\n",
    "\n",
    "    # Print imported row counts for all tables\n",
    "    print(\"\\nImported row counts:\")\n",
    "    for tbl in all_tables:\n",
    "        try:\n",
    "            count = con.execute(f\"SELECT COUNT(*) FROM {tbl}\").fetchone()[0]\n",
    "            print(f\"{tbl}: {count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{tbl}: Error getting row count ({e})\")\n",
    "\n",
    "    # 3. Create a virtual view (joined table)\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE VIEW NSE_Master AS\n",
    "        SELECT a.*, b.*\n",
    "        FROM All_Nifty_Indices_Meta_Data a\n",
    "        LEFT JOIN nse_symbols b\n",
    "        ON a.\"Symbol\" = b.\"SYMBOL\"\n",
    "    \"\"\")\n",
    "    print(\"\\nâœ… NSE_Master view created (join of All_Nifty_Indices_Meta_Data and nse_symbols).\")\n",
    "\n",
    "    # 4. Show 30 rows from NSE_Master view\n",
    "    print(\"\\nFirst 30 rows from NSE_Master view:\")\n",
    "    df_master = con.execute('SELECT * FROM NSE_Master LIMIT 30').fetchdf()\n",
    "    print(df_master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ded02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "bse_indices_csv = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\All stocks Meta Data files\\All BSE Indices Meta Data.csv\"\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "# Read CSV\n",
    "df_indices = pd.read_csv(bse_indices_csv)\n",
    "df_indices.columns = [c.strip() for c in df_indices.columns]\n",
    "\n",
    "# --- Find code columns for join ---\n",
    "def find_code(colnames):\n",
    "    \"\"\"Finds likely code column from standard BSE alternatives.\"\"\"\n",
    "    for key in [\"Scrip Code\", \"SC_CODE\", \"Sc_code\", \"Security Code\", \"Security_Code\", \"sc_code\"]:\n",
    "        if key in colnames:\n",
    "            return key\n",
    "    # Try lower-case search\n",
    "    for key in colnames:\n",
    "        if key.lower().replace(\"_\", \" \").find(\"code\") != -1:\n",
    "            return key\n",
    "    raise ValueError(\"No code column found for join!\")\n",
    "\n",
    "idx_code = find_code(df_indices.columns)\n",
    "\n",
    "with duckdb.connect(db_path) as con:\n",
    "    # Write/overwrite index table\n",
    "    con.execute(\"DROP TABLE IF EXISTS All_BSE_Indices_Meta_Data\")\n",
    "    con.execute(\"CREATE TABLE All_BSE_Indices_Meta_Data AS SELECT * FROM df_indices\")\n",
    "\n",
    "    # Get canonical BSE tables (you should already have bse_symbols and bse_metadata)\n",
    "    all_tables = con.execute(\"SHOW TABLES\").fetchdf()['name'].tolist()\n",
    "    print(\"\\nTables currently in DB:\", all_tables)\n",
    "\n",
    "    # Print imported row counts\n",
    "    print(\"\\nImported rows:\")\n",
    "    for table in all_tables:\n",
    "        try:\n",
    "            count = con.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "            print(f\"{table}: {count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{table}: Error getting row count ({e})\")\n",
    "\n",
    "    # Dynamically find code columns for symbols and meta\n",
    "    sym_code = find_code(con.execute(\"PRAGMA table_info('bse_symbols')\").fetchdf()['name'])\n",
    "    meta_codes = []\n",
    "    meta_cols = con.execute(\"PRAGMA table_info('bse_metadata')\").fetchdf()['name']\n",
    "    for code_cand in [\"Scrip Code\", \"Security Code\", \"SC_CODE\", \"Sc_code\", \"Security_Code\", \"sc_code\"]:\n",
    "        if code_cand in meta_cols.values:\n",
    "            meta_codes.append(code_cand)\n",
    "    if not meta_codes:\n",
    "        raise ValueError(\"No code column found in bse_metadata table!\")\n",
    "    meta_on = \" OR \".join(\n",
    "        [f'CAST(idx.\"{idx_code}\" AS VARCHAR) = CAST(meta.\"{mc}\" AS VARCHAR)' for mc in meta_codes]\n",
    "    )\n",
    "\n",
    "    # Create the view for INNER JOIN on all code columns\n",
    "    join_sql = f\"\"\"\n",
    "        CREATE OR REPLACE VIEW BSE_Master AS\n",
    "        SELECT idx.*, sym.*, meta.*\n",
    "        FROM All_BSE_Indices_Meta_Data idx\n",
    "        RIGHT JOIN bse_symbols sym\n",
    "            ON CAST(idx.\"{idx_code}\" AS VARCHAR) = CAST(sym.\"{sym_code}\" AS VARCHAR)\n",
    "        RIGHT JOIN bse_metadata meta\n",
    "            ON {meta_on}\n",
    "    \"\"\"\n",
    "    con.execute(join_sql)\n",
    "    print(\"\\nâœ… BSE_Master view created (INNER JOIN on code columns across all tables).\")\n",
    "\n",
    "    # Show 20 sample rows from the virtual table BSE_Master\n",
    "    print(\"\\nSample rows from BSE_Master (20 rows):\")\n",
    "    df_master = con.execute(\"SELECT * FROM BSE_Master LIMIT 20\").fetchdf()\n",
    "    print(df_master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import xlwings as xw\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def update_symbol_db(write_excel=True):\n",
    "    # ==== DYNAMIC PATH SETUP ====\n",
    "    # Always point to the Rubik_view root, so any script location works.\n",
    "    RUBIK_ROOT = Path(r\"C:/Users/jallu/OneDrive/pgp/Python/Stock predictor/Rubik_view\")\n",
    "    DATA = RUBIK_ROOT / \"Data\" / \"Symbols Data\"\n",
    "    META = DATA / \"All stocks Meta Data files\"\n",
    "    db_path = DATA / \"symbols.duckdb\"\n",
    "\n",
    "    today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # === Load NSE (live download)\n",
    "    nse_eq_url = \"https://archives.nseindia.com/content/equities/EQUITY_L.csv\"\n",
    "    df_nse = pd.read_csv(nse_eq_url, skipinitialspace=True)\n",
    "    df_nse.columns = df_nse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_nse['MISSING_DATA'] = df_nse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    nse_loaded_count = len(df_nse)\n",
    "\n",
    "    # === Load BSE Master & Meta from CSV\n",
    "    bse_csv_path = DATA / \"Equity.csv\"\n",
    "    bhavcopy_path = DATA / \"bhavcopy.csv\"\n",
    "    df_bse = pd.read_csv(bse_csv_path, skipinitialspace=True)\n",
    "    df_bse.columns = df_bse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse['MISSING_DATA'] = df_bse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bse_loaded_count = len(df_bse)\n",
    "\n",
    "    df_bse_meta = pd.read_csv(bhavcopy_path, skipinitialspace=True)\n",
    "    df_bse_meta.columns = df_bse_meta.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse_meta['MISSING_DATA'] = df_bse_meta.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bsemeta_loaded_count = len(df_bse_meta)\n",
    "\n",
    "    # === Load All Nifty Indices Meta Data ===\n",
    "    nifty_indices_path = META / \"All Nifty Indices Meta Data.csv\"\n",
    "    df_nifty = pd.read_csv(nifty_indices_path)\n",
    "    df_nifty.columns = [col.strip() for col in df_nifty.columns]\n",
    "    nifty_count = len(df_nifty)\n",
    "\n",
    "    # === Load All BSE Indices Meta Data ===\n",
    "    bse_indices_path = META / \"All BSE Indices Meta Data.csv\"\n",
    "    df_bse_indices = pd.read_csv(bse_indices_path)\n",
    "    df_bse_indices.columns = [col.strip() for col in df_bse_indices.columns]\n",
    "    bse_indices_count = len(df_bse_indices)\n",
    "\n",
    "    # === Write to DuckDB ===\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        # NSE/BSE main tables\n",
    "        con.execute(\"DROP TABLE IF EXISTS nse_symbols\")\n",
    "        con.execute(\"CREATE TABLE nse_symbols AS SELECT * FROM df_nse\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS bse_symbols\")\n",
    "        con.execute(\"CREATE TABLE bse_symbols AS SELECT * FROM df_bse\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS bse_metadata\")\n",
    "        con.execute(\"CREATE TABLE bse_metadata AS SELECT * FROM df_bse_meta\")\n",
    "\n",
    "        # Nifty/BSE indices\n",
    "        con.execute(\"DROP TABLE IF EXISTS All_Nifty_Indices_Meta_Data\")\n",
    "        con.execute(\"CREATE TABLE All_Nifty_Indices_Meta_Data AS SELECT * FROM df_nifty\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS All_BSE_Indices_Meta_Data\")\n",
    "        con.execute(\"CREATE TABLE All_BSE_Indices_Meta_Data AS SELECT * FROM df_bse_indices\")\n",
    "\n",
    "        # Print imported row counts in terminal (including virtuals here)\n",
    "        all_tables = con.execute(\"SHOW TABLES\").fetchdf()['name'].tolist()\n",
    "        print(\"\\nTables currently in DB:\", all_tables)\n",
    "        print(\"\\nImported row counts:\")\n",
    "        for tbl in all_tables:\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {tbl}\").fetchone()[0]\n",
    "                print(f\"{tbl}: {count}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{tbl}: Error getting row count ({e})\")\n",
    "\n",
    "        # --- Create NSE_Master view (LEFT JOIN) ---\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW NSE_Master AS\n",
    "            SELECT a.*, b.*\n",
    "            FROM All_Nifty_Indices_Meta_Data a\n",
    "            LEFT JOIN nse_symbols b\n",
    "            ON a.\"Symbol\" = b.\"SYMBOL\"\n",
    "        \"\"\")\n",
    "        print(\"\\nâœ… NSE_Master view created (join of All_Nifty_Indices_Meta_Data and nse_symbols).\")\n",
    "\n",
    "        # --- Create BSE_Master view (join all available code columns) ---\n",
    "        def find_code(colnames):\n",
    "            for key in [\"Scrip Code\", \"SC_CODE\", \"Sc_code\", \"Security Code\", \"Security_Code\", \"sc_code\"]:\n",
    "                if key in colnames:\n",
    "                    return key\n",
    "            for key in colnames:\n",
    "                if key.lower().replace(\"_\", \" \").find(\"code\") != -1:\n",
    "                    return key\n",
    "            raise ValueError(\"No code column found for join!\")\n",
    "\n",
    "        idx_code = find_code(df_bse_indices.columns)\n",
    "        sym_code = find_code(con.execute(\"PRAGMA table_info('bse_symbols')\").fetchdf()['name'])\n",
    "        meta_codes = []\n",
    "        meta_cols = con.execute(\"PRAGMA table_info('bse_metadata')\").fetchdf()['name']\n",
    "        for code_cand in [\"Scrip Code\", \"Security Code\", \"SC_CODE\", \"Sc_code\", \"Security_Code\", \"sc_code\"]:\n",
    "            if code_cand in meta_cols.values:\n",
    "                meta_codes.append(code_cand)\n",
    "        if not meta_codes:\n",
    "            raise ValueError(\"No code column found in bse_metadata table!\")\n",
    "        meta_on = \" OR \".join(\n",
    "            [f'CAST(idx.\"{idx_code}\" AS VARCHAR) = CAST(meta.\"{mc}\" AS VARCHAR)' for mc in meta_codes]\n",
    "        )\n",
    "\n",
    "        join_sql = f\"\"\"\n",
    "            CREATE OR REPLACE VIEW BSE_Master AS\n",
    "            SELECT idx.*, sym.*, meta.*\n",
    "            FROM All_BSE_Indices_Meta_Data idx\n",
    "            RIGHT JOIN bse_symbols sym\n",
    "                ON CAST(idx.\"{idx_code}\" AS VARCHAR) = CAST(sym.\"{sym_code}\" AS VARCHAR)\n",
    "            RIGHT JOIN bse_metadata meta\n",
    "                ON {meta_on}\n",
    "        \"\"\"\n",
    "        con.execute(join_sql)\n",
    "        print(\"\\nâœ… BSE_Master view created (joined on code columns across all tables).\")\n",
    "\n",
    "        # Print 20 rows of each master view\n",
    "        print(\"\\nFirst 20 rows from NSE_Master view:\")\n",
    "        df_nse_master = con.execute('SELECT * FROM NSE_Master LIMIT 20').fetchdf()\n",
    "        print(df_nse_master)\n",
    "\n",
    "        print(\"\\nFirst 20 rows from BSE_Master view:\")\n",
    "        df_bse_master = con.execute('SELECT * FROM BSE_Master LIMIT 20').fetchdf()\n",
    "        print(df_bse_master)\n",
    "\n",
    "        # --- Prepare summary for Excel (skip virtuals) ---\n",
    "        skip_virtuals = {'NSE_Master', 'BSE_Master'}\n",
    "        summary = []\n",
    "        for tbl in all_tables:\n",
    "            if tbl in skip_virtuals:\n",
    "                continue\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {tbl}\").fetchone()[0]\n",
    "                summary.append([tbl, count, today_str])\n",
    "            except Exception as e:\n",
    "                summary.append([tbl, f\"Error: {e}\", today_str])\n",
    "\n",
    "    # --- Write to Excel if triggered ---\n",
    "    if write_excel:\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "            sht = wb.sheets[\"Update Dash board\"]\n",
    "            sht.range(\"H2\").value = [[\"Table\", \"Imported Count\", \"Updated Date\"]] + summary\n",
    "        except Exception as e:\n",
    "            print(\"\\n[Excel output skipped: not run from Excel or sheet missing.]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For standalone Python/terminal runs, skip Excel output:\n",
    "    update_symbol_db(write_excel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563438ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "def load_symbols():\n",
    "    SYMBOLS_DB = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\")\n",
    "    conn = duckdb.connect(str(SYMBOLS_DB))\n",
    "\n",
    "    # NSE: From NSE_Master.Symbol\n",
    "    nse = conn.execute(\"SELECT DISTINCT Symbol FROM NSE_Master WHERE Symbol IS NOT NULL\").fetchdf()\n",
    "    nse_syms = nse['Symbol'].astype(str).str.strip() + \".NS\"\n",
    "\n",
    "    # BSE: From BSE_Master.SECURITY_CODE\n",
    "    bse = conn.execute(\"SELECT DISTINCT SECURITY_CODE FROM BSE_Master WHERE SECURITY_CODE IS NOT NULL\").fetchdf()\n",
    "    bse_syms = bse['SECURITY_CODE'].astype(str).str.strip() + \".BO\"\n",
    "\n",
    "    all_syms = pd.concat([nse_syms, bse_syms]).dropna().unique().tolist()\n",
    "    conn.close()\n",
    "    return all_syms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import xlwings as xw\n",
    "import numpy as np\n",
    "\n",
    "def classify_signal(score, min_score, max_score):\n",
    "    if max_score == min_score:\n",
    "        norm_val = 0\n",
    "    else:\n",
    "        norm_val = (score - min_score) / (max_score - min_score) * 20 - 10\n",
    "    if norm_val >= 7:\n",
    "        sig = \"Extreem Bullish â¬†ï¸â¬†ï¸ \"\n",
    "    elif norm_val >= 3:\n",
    "        sig = \"Bullishâ¬†ï¸\"\n",
    "    elif norm_val > -3:\n",
    "        sig = \"Hold â†”ï¸\"\n",
    "    elif norm_val > -7:\n",
    "        sig = \"Bearish â¬‡ï¸\"\n",
    "    else:\n",
    "        sig = \"Extreem Bearish â¬‡ï¸â¬‡ï¸\"\n",
    "    return round(norm_val, 2), sig\n",
    "\n",
    "# --- Robust NSE Mapping ---\n",
    "def get_nse_symbol_map():\n",
    "    SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "    nse_map = {}\n",
    "    with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "        nse_df = con.execute(\"SELECT SYMBOL, NAME_OF_COMPANY FROM nse_symbols\").fetchdf()\n",
    "        for _, row in nse_df.iterrows():\n",
    "            nse_map[str(row['SYMBOL']).strip().upper()] = row['NAME_OF_COMPANY']\n",
    "    return nse_map\n",
    "\n",
    "# --- Robust BSE Mapping ---\n",
    "def get_bse_symbol_map():\n",
    "    SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "    bse_map = {}\n",
    "    with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "        # Join metadata with master table using SECURITY_CODE\n",
    "        join_df = con.execute(\"\"\"\n",
    "            SELECT meta.SC_NAME, meta.SC_CODE, sym.SECURITY_ID, sym.ISSUER_NAME\n",
    "            FROM bse_metadata AS meta\n",
    "            JOIN bse_symbols AS sym\n",
    "              ON CAST(meta.SC_CODE AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR)\n",
    "            WHERE meta.SC_NAME IS NOT NULL\n",
    "        \"\"\").fetchdf()\n",
    "        for _, row in join_df.iterrows():\n",
    "            sc_name = str(row['SC_NAME']).strip().upper()\n",
    "            sc_code = str(row['SC_CODE']).strip().upper()\n",
    "            security_id = str(row['SECURITY_ID']).strip().upper()\n",
    "            issuer_name = row['ISSUER_NAME']\n",
    "            bse_map[sc_name] = issuer_name\n",
    "            bse_map[sc_code] = issuer_name\n",
    "            bse_map[security_id] = issuer_name\n",
    "\n",
    "        # Also add: map SECURITY_CODE and SECURITY_ID directly for lookup/fallback\n",
    "        direct_df = con.execute(\"SELECT SECURITY_CODE, SECURITY_ID, ISSUER_NAME FROM bse_symbols\").fetchdf()\n",
    "        for _, row in direct_df.iterrows():\n",
    "            code = str(row['SECURITY_CODE']).strip().upper()\n",
    "            sid  = str(row['SECURITY_ID']).strip().upper()\n",
    "            name = row['ISSUER_NAME']\n",
    "            if code not in bse_map:\n",
    "                bse_map[code] = name\n",
    "            if sid not in bse_map:\n",
    "                bse_map[sid] = name\n",
    "\n",
    "    return bse_map\n",
    "\n",
    "\n",
    "# --- Universal Symbol Resolver ---\n",
    "def resolve_name(symbol, nse_map, bse_map):\n",
    "    sym = str(symbol).strip().upper()\n",
    "    if sym.endswith('.NS'):\n",
    "        base = sym[:-3]\n",
    "        return nse_map.get(base, \"\") or \"\"\n",
    "    elif sym.endswith('.BO'):\n",
    "        base = sym[:-3]\n",
    "        return bse_map.get(base, \"\") or bse_map.get(sym, \"\") or \"\"\n",
    "    else:\n",
    "        return nse_map.get(sym, \"\") or bse_map.get(sym, \"\") or \"\"\n",
    "\n",
    "def main():\n",
    "    BASE_DIR = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\"\n",
    "    SIGNALS_DB = f\"{BASE_DIR}\\\\Data\\\\Signals Data\\\\signals.duckdb\"\n",
    "    INDICATOR_SHEET = \"Indicators\"\n",
    "    OUTPUT_SHEET = \"Auto Stock Selection\"\n",
    "\n",
    "    wb = xw.Book.caller()\n",
    "    ind_sheet = wb.sheets[INDICATOR_SHEET]\n",
    "    out_sheet = wb.sheets[OUTPUT_SHEET]\n",
    "\n",
    "    out_sheet.range(\"N4\").value = \"Input Count\"\n",
    "    try:\n",
    "        n_picks = int(out_sheet.range(\"O4\").value)\n",
    "        if n_picks <= 0:\n",
    "            n_picks = 10\n",
    "    except:\n",
    "        n_picks = 10\n",
    "\n",
    "    # --- Load only active indicators ---\n",
    "    ind_df = ind_sheet.range(\"A1\").options(pd.DataFrame, header=1, index=False, expand='table').value\n",
    "    ind_df.columns = ind_df.columns.str.strip().str.lower()\n",
    "    col_active = [col for col in ind_df.columns if 'active' in col][0]\n",
    "    col_name = [col for col in ind_df.columns if 'indicator' in col and 'name' in col][0]\n",
    "    col_weight = [col for col in ind_df.columns if 'weight' in col and 'manual' in col][0]\n",
    "    active = ind_df[ind_df[col_active].str.upper() == 'Y']\n",
    "    weights = dict(zip(active[col_name], active[col_weight]))\n",
    "\n",
    "    # --- Indicator count and categories summary ---\n",
    "    out_sheet.range(\"O7\").value = len(active)\n",
    "    out_sheet.range(\"N7\").value = \"Indicators\"\n",
    "    categories = active['category'].dropna().unique()\n",
    "    category_counts = active['category'].value_counts()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        n_row = 8 + idx\n",
    "        out_sheet.range(f\"N{n_row}\").value = cat\n",
    "        out_sheet.range(f\"O{n_row}\").value = int(category_counts[cat])\n",
    "\n",
    "    # --- Fetch latest signals (only for active indicators) ---\n",
    "    with duckdb.connect(SIGNALS_DB, read_only=True) as con:\n",
    "        cols = con.execute(\"PRAGMA table_info('signals')\").fetchdf()\n",
    "        act_sig_cols = [\n",
    "            indicator.lower().replace(\" \", \"_\").replace(\"%\", \"pct\") + \"_signal\"\n",
    "            for indicator in weights\n",
    "        ]\n",
    "        act_sig_cols = [c for c in act_sig_cols if c in cols['name'].values]\n",
    "        select_cols = ['symbol', 'date'] + act_sig_cols\n",
    "        if not act_sig_cols:\n",
    "            out_sheet.range(\"A:E\").clear_contents()\n",
    "            out_sheet.range(\"A1\").options(index=False).value = [[\"Name\", \"Symbol\", \"Score\", \"Signal Range (-10 to 10)\", \"Signal Text\"]]\n",
    "            print(\"No active indicator signals found in DB!\")\n",
    "            return\n",
    "        df = con.execute(f\"SELECT {', '.join(select_cols)} FROM signals\").fetchdf()\n",
    "\n",
    "    df = df.sort_values('date').groupby('symbol').tail(1).reset_index(drop=True)\n",
    "\n",
    "    # --- Compute scores using only active indicator columns ---\n",
    "    score_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        total_score = 0\n",
    "        for indicator, weight in weights.items():\n",
    "            ind_key = indicator.lower().replace(\" \", \"_\").replace(\"%\", \"pct\") + \"_signal\"\n",
    "            signal = row.get(ind_key, None)\n",
    "            if signal is None or pd.isnull(signal):\n",
    "                continue\n",
    "            try:\n",
    "                score = float(signal) * float(weight)\n",
    "                total_score += score\n",
    "            except:\n",
    "                continue\n",
    "        score_list.append({'Symbol': row['symbol'], 'Score': total_score})\n",
    "\n",
    "    score_df = pd.DataFrame(score_list)\n",
    "    if score_df.empty:\n",
    "        out_sheet.range(\"A:E\").clear_contents()\n",
    "        out_sheet.range(\"A1\").options(index=False).value = [[\"Name\", \"Symbol\", \"Score\", \"Signal Range (-10 to 10)\", \"Signal Text\"]]\n",
    "        print(\"No scores computed.\")\n",
    "        return\n",
    "\n",
    "    # --- Normalize and assign signal text ---\n",
    "    score_min = score_df['Score'].min()\n",
    "    score_max = score_df['Score'].max()\n",
    "    norm_vals, signal_texts = [], []\n",
    "    for s in score_df['Score']:\n",
    "        norm, sig = classify_signal(s, score_min, score_max)\n",
    "        norm_vals.append(norm)\n",
    "        signal_texts.append(sig)\n",
    "    score_df['Signal Range (-10 to 10)'] = norm_vals\n",
    "    score_df['Signal Text'] = signal_texts\n",
    "\n",
    "    # --- Add Name column using robust mapping ---\n",
    "    nse_map = get_nse_symbol_map()\n",
    "    bse_map = get_bse_symbol_map()\n",
    "    score_df['Name'] = score_df['Symbol'].apply(lambda s: resolve_name(s, nse_map, bse_map))\n",
    "\n",
    "    # --- Sort by abs value of normalized score, pick top N ---\n",
    "    score_df['abs_score'] = score_df['Signal Range (-10 to 10)'].abs()\n",
    "    topn = score_df.sort_values('abs_score', ascending=False).head(n_picks)\n",
    "    topn = topn[['Name', 'Symbol', 'Score', 'Signal Range (-10 to 10)', 'Signal Text']]\n",
    "\n",
    "    # --- Output to sheet (columns A-E) ---\n",
    "    out_sheet.range(\"A:E\").clear_contents()\n",
    "    out_sheet.range(\"A1\").options(index=False).value = topn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xw.Book(\"rubikview.xlsm\").set_mock_caller()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac6ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported BSE_Master to Excel: C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\BSE_Master.xlsx\n",
      "âœ… Exported BSE_Master to CSV: C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\BSE_Master.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "EXPORT_PATH_EXCEL = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\BSE_Master.xlsx\"\n",
    "EXPORT_PATH_CSV = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\BSE_Master.csv\"\n",
    "\n",
    "# Fetch all data from BSE_Master\n",
    "with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "    df_bse_master = con.execute(\"SELECT * FROM BSE_Master\").fetchdf()\n",
    "\n",
    "# Export to Excel\n",
    "df_bse_master.to_excel(EXPORT_PATH_EXCEL, index=False)\n",
    "print(f\"âœ… Exported BSE_Master to Excel: {EXPORT_PATH_EXCEL}\")\n",
    "\n",
    "# Export to CSV\n",
    "df_bse_master.to_csv(EXPORT_PATH_CSV, index=False)\n",
    "print(f\"âœ… Exported BSE_Master to CSV: {EXPORT_PATH_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4eaff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\jallu\\\\OneDrive\\\\pgp\\\\Python\\\\Stock predictor\\\\Rubik_view\\\\Data\\\\NSE_Master.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(SYMBOLS_DB, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM NSE_Master\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[1;32m---> 10\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(CSV_EXPORT_PATH, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Exported NSE_Master to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_EXPORT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jallu\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jallu\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jallu\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\jallu\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\jallu\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\jallu\\\\OneDrive\\\\pgp\\\\Python\\\\Stock predictor\\\\Rubik_view\\\\Data\\\\NSE_Master.csv'"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "CSV_EXPORT_PATH = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\NSE_Master.csv\"\n",
    "\n",
    "with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "    df = con.execute(\"SELECT * FROM NSE_Master\").fetchdf()\n",
    "\n",
    "df.to_csv(CSV_EXPORT_PATH, index=False)\n",
    "print(f\"âœ… Exported NSE_Master to {CSV_EXPORT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f9bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported to C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\bse_master_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    COALESCE(\"Scrip Code\", SC_CODE, SECURITY_CODE)            AS SYMBOL_CODE,\n",
    "    COALESCE(COMPANY, ISSUER_NAME)                            AS COMPANY_NAME,\n",
    "    COALESCE(SECURITY_ID, SC_NAME)                            AS SYMBOL_NAME,\n",
    "    IndexName,\n",
    "    SECTOR_NAME,\n",
    "    COALESCE(INDUSTRY_NEW_NAME, INDUSTRY_1)                   AS INDUSTRY,\n",
    "    INSTRUMENT\n",
    "FROM BSE_Master\n",
    "\"\"\"\n",
    "\n",
    "with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "    bse_df = con.execute(query).fetchdf()\n",
    "\n",
    "# Export to CSV\n",
    "output_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\bse_master_cleaned.csv\"\n",
    "bse_df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Exported to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c50c648",
   "metadata": {},
   "outputs": [
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Table \"idx\" does not have a column named \"SC_CODE\"\n\nCandidate bindings: : \"Scrip Code\"\n\nLINE 6:             ON CAST(idx.SC_CODE AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR...\n                            ^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m SYMBOLS_DB \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjallu\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpgp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mStock predictor\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRubik_view\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSymbols Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msymbols.duckdb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 1: Create/replace the main BSE_Master view with correct joins\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(SYMBOLS_DB) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m     10\u001b[0m     con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m        CREATE OR REPLACE VIEW BSE_Master AS\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m        SELECT idx.*, sym.*, meta.*\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m            ON CAST(idx.SC_CODE AS VARCHAR) = CAST(meta.SC_CODE AS VARCHAR)\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… BSE_Master view created/joined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 1: Create/replace the main BSE_Master view with correct joins\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(SYMBOLS_DB) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m---> 10\u001b[0m     con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m        CREATE OR REPLACE VIEW BSE_Master AS\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m        SELECT idx.*, sym.*, meta.*\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m        FROM All_BSE_Indices_Meta_Data idx\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m        FULL OUTER JOIN bse_symbols sym\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m            ON CAST(idx.SC_CODE AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR)\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m        FULL OUTER JOIN bse_metadata meta\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m            ON CAST(idx.SC_CODE AS VARCHAR) = CAST(meta.SC_CODE AS VARCHAR)\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… BSE_Master view created/joined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Step 2: Create a *cleaned* virtual view for export, using COALESCE on columns as required\u001b[39;00m\n",
      "\u001b[1;31mBinderException\u001b[0m: Binder Error: Table \"idx\" does not have a column named \"SC_CODE\"\n\nCandidate bindings: : \"Scrip Code\"\n\nLINE 6:             ON CAST(idx.SC_CODE AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR...\n                            ^"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your DuckDB database\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "# Step 1: Create/replace the main BSE_Master view with correct joins\n",
    "with duckdb.connect(SYMBOLS_DB) as con:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE VIEW BSE_Master AS\n",
    "        SELECT idx.*, sym.*, meta.*\n",
    "        FROM All_BSE_Indices_Meta_Data idx\n",
    "        FULL OUTER JOIN bse_symbols sym\n",
    "            ON CAST(idx.SC_CODE AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR)\n",
    "        FULL OUTER JOIN bse_metadata meta\n",
    "            ON CAST(idx.SC_CODE AS VARCHAR) = CAST(meta.SC_CODE AS VARCHAR)\n",
    "    \"\"\")\n",
    "    print(\"âœ… BSE_Master view created/joined.\")\n",
    "\n",
    "    # Step 2: Create a *cleaned* virtual view for export, using COALESCE on columns as required\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE VIEW BSE_Master_Cleaned AS\n",
    "        SELECT\n",
    "            COALESCE(\"Scrip Code\", SC_CODE, SECURITY_CODE)    AS SYMBOL_CODE,\n",
    "            COALESCE(COMPANY, ISSUER_NAME)                    AS COMPANY_NAME,\n",
    "            COALESCE(SECURITY_ID, SC_NAME)                    AS SYMBOL_NAME,\n",
    "            IndexName,\n",
    "            SECTOR_NAME,\n",
    "            COALESCE(INDUSTRY_NEW_NAME, INDUSTRY_1)           AS INDUSTRY,\n",
    "            INSTRUMENT\n",
    "        FROM BSE_Master\n",
    "    \"\"\")\n",
    "    print(\"âœ… BSE_Master_Cleaned view created.\")\n",
    "\n",
    "    # Step 3: Export the virtual view to CSV\n",
    "    cleaned_df = con.execute(\"SELECT * FROM BSE_Master_Cleaned\").fetchdf()\n",
    "    output_path = Path(SYMBOLS_DB).parent / \"bse_master_cleaned.csv\"\n",
    "    cleaned_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Exported cleaned BSE master view to: {output_path}\")\n",
    "\n",
    "    # Optional: If you want to check a sample\n",
    "    print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567fa8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… NSE_Master view created (join of All_Nifty_Indices_Meta_Data and nse_symbols).\n",
      "âœ… Exported NSE_Master view to: C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\nse_master.csv\n",
      "       Company Name            Industry  Symbol Series     ISIN Code  \\\n",
      "0  360 ONE WAM Ltd.  Financial Services  360ONE     EQ  INE466L01038   \n",
      "1  360 ONE WAM Ltd.  Financial Services  360ONE     EQ  INE466L01038   \n",
      "2  360 ONE WAM Ltd.  Financial Services  360ONE     EQ  INE466L01038   \n",
      "3  360 ONE WAM Ltd.  Financial Services  360ONE     EQ  INE466L01038   \n",
      "4  360 ONE WAM Ltd.  Financial Services  360ONE     EQ  INE466L01038   \n",
      "\n",
      "                               IndexName  Unnamed: 6 SYMBOL_1  \\\n",
      "0  nifty500LargeMidSmallEqualCapWeighted         NaN   360ONE   \n",
      "1                               nifty500         NaN   360ONE   \n",
      "2                 nifty500Multicap502525         NaN   360ONE   \n",
      "3                    niftylargemidcap250         NaN   360ONE   \n",
      "4                         niftymidcap150         NaN   360ONE   \n",
      "\n",
      "       NAME_OF_COMPANY SERIES_1 DATE_OF_LISTING  PAID_UP_VALUE  MARKET_LOT  \\\n",
      "0  360 ONE WAM LIMITED       EQ     19-SEP-2019              1           1   \n",
      "1  360 ONE WAM LIMITED       EQ     19-SEP-2019              1           1   \n",
      "2  360 ONE WAM LIMITED       EQ     19-SEP-2019              1           1   \n",
      "3  360 ONE WAM LIMITED       EQ     19-SEP-2019              1           1   \n",
      "4  360 ONE WAM LIMITED       EQ     19-SEP-2019              1           1   \n",
      "\n",
      "    ISIN_NUMBER  FACE_VALUE MISSING_DATA  \n",
      "0  INE466L01038           1           No  \n",
      "1  INE466L01038           1           No  \n",
      "2  INE466L01038           1           No  \n",
      "3  INE466L01038           1           No  \n",
      "4  INE466L01038           1           No  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your DuckDB database\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "with duckdb.connect(SYMBOLS_DB) as con:\n",
    "    # --- Create NSE_Master view (FULL OUTER JOIN) ---\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE VIEW NSE_Master AS\n",
    "        SELECT a.*, b.*\n",
    "        FROM All_Nifty_Indices_Meta_Data a\n",
    "        FULL OUTER JOIN nse_symbols b\n",
    "        ON a.\"Symbol\" = b.\"SYMBOL\"\n",
    "    \"\"\")\n",
    "    print(\"\\nâœ… NSE_Master view created (join of All_Nifty_Indices_Meta_Data and nse_symbols).\")\n",
    "\n",
    "    # --- Export NSE_Master view to CSV ---\n",
    "    nse_master_df = con.execute(\"SELECT * FROM NSE_Master\").fetchdf()\n",
    "    nse_output_path = Path(SYMBOLS_DB).parent / \"nse_master.csv\"\n",
    "    nse_master_df.to_csv(nse_output_path, index=False)\n",
    "    print(f\"âœ… Exported NSE_Master view to: {nse_output_path}\")\n",
    "\n",
    "    # Optional: Show top few rows\n",
    "    print(nse_master_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6eca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NSE_Master_Cleaned view created.\n",
      "âœ… Exported cleaned NSE master view to: C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\nse_master_cleaned.csv\n",
      "   SYMBOL      COMPANY_NAME            INDUSTRY  \\\n",
      "0  360ONE  360 ONE WAM Ltd.  Financial Services   \n",
      "1  360ONE  360 ONE WAM Ltd.  Financial Services   \n",
      "2  360ONE  360 ONE WAM Ltd.  Financial Services   \n",
      "3  360ONE  360 ONE WAM Ltd.  Financial Services   \n",
      "4  360ONE  360 ONE WAM Ltd.  Financial Services   \n",
      "\n",
      "                              INDEX_NAME  \n",
      "0  nifty500LargeMidSmallEqualCapWeighted  \n",
      "1                               nifty500  \n",
      "2                 nifty500Multicap502525  \n",
      "3                    niftylargemidcap250  \n",
      "4                         niftymidcap150  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your DuckDB database\n",
    "SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "\n",
    "with duckdb.connect(SYMBOLS_DB) as con:\n",
    "    # --- Create a cleaned NSE master view with harmonized columns ---\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE VIEW NSE_Master_Cleaned AS\n",
    "        SELECT\n",
    "            COALESCE(\"Symbol\", SYMBOL_1)                    AS SYMBOL,\n",
    "            COALESCE(\"Company Name\", NAME_OF_COMPANY)        AS COMPANY_NAME,\n",
    "            Industry                                         AS INDUSTRY,\n",
    "            IndexName                                        AS INDEX_NAME\n",
    "        FROM NSE_Master\n",
    "    \"\"\")\n",
    "    print(\"âœ… NSE_Master_Cleaned view created.\")\n",
    "\n",
    "    # --- Export the cleaned view to CSV ---\n",
    "    nse_cleaned_df = con.execute(\"SELECT * FROM NSE_Master_Cleaned\").fetchdf()\n",
    "    nse_cleaned_path = Path(SYMBOLS_DB).parent / \"nse_master_cleaned.csv\"\n",
    "    nse_cleaned_df.to_csv(nse_cleaned_path, index=False)\n",
    "    print(f\"âœ… Exported cleaned NSE master view to: {nse_cleaned_path}\")\n",
    "\n",
    "    # Optional: Display first few rows\n",
    "    print(nse_cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import xlwings as xw\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def update_symbol_db(write_excel=True):\n",
    "    # ==== DYNAMIC PATH SETUP ====\n",
    "    RUBIK_ROOT = Path(r\"C:/Users/jallu/OneDrive/pgp/Python/Stock predictor/Rubik_view\")\n",
    "    DATA = RUBIK_ROOT / \"Data\" / \"Symbols Data\"\n",
    "    META = DATA / \"All stocks Meta Data files\"\n",
    "    db_path = DATA / \"symbols.duckdb\"\n",
    "\n",
    "    today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # === Load NSE (live download)\n",
    "    nse_eq_url = \"https://archives.nseindia.com/content/equities/EQUITY_L.csv\"\n",
    "    df_nse = pd.read_csv(nse_eq_url, skipinitialspace=True)\n",
    "    df_nse.columns = df_nse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_nse['MISSING_DATA'] = df_nse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    nse_loaded_count = len(df_nse)\n",
    "\n",
    "    # === Load BSE Master & Meta from CSV\n",
    "    bse_csv_path = DATA / \"Equity.csv\"\n",
    "    bhavcopy_path = DATA / \"bhavcopy.csv\"\n",
    "    df_bse = pd.read_csv(bse_csv_path, skipinitialspace=True)\n",
    "    df_bse.columns = df_bse.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse['MISSING_DATA'] = df_bse.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bse_loaded_count = len(df_bse)\n",
    "\n",
    "    df_bse_meta = pd.read_csv(bhavcopy_path, skipinitialspace=True)\n",
    "    df_bse_meta.columns = df_bse_meta.columns.str.strip().str.upper().str.replace(\" \", \"_\")\n",
    "    df_bse_meta['MISSING_DATA'] = df_bse_meta.isnull().any(axis=1).map({True: 'Yes', False: 'No'})\n",
    "    bsemeta_loaded_count = len(df_bse_meta)\n",
    "\n",
    "    # === Load All Nifty Indices Meta Data ===\n",
    "    nifty_indices_path = META / \"All Nifty Indices Meta Data.csv\"\n",
    "    df_nifty = pd.read_csv(nifty_indices_path)\n",
    "    df_nifty.columns = [col.strip() for col in df_nifty.columns]\n",
    "    nifty_count = len(df_nifty)\n",
    "\n",
    "    # === Load All BSE Indices Meta Data ===\n",
    "    bse_indices_path = META / \"All BSE Indices Meta Data.csv\"\n",
    "    df_bse_indices = pd.read_csv(bse_indices_path)\n",
    "    df_bse_indices.columns = [col.strip() for col in df_bse_indices.columns]\n",
    "    bse_indices_count = len(df_bse_indices)\n",
    "\n",
    "    # === Write to DuckDB ===\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        # NSE/BSE main tables\n",
    "        con.execute(\"DROP TABLE IF EXISTS nse_symbols\")\n",
    "        con.execute(\"CREATE TABLE nse_symbols AS SELECT * FROM df_nse\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS bse_symbols\")\n",
    "        con.execute(\"CREATE TABLE bse_symbols AS SELECT * FROM df_bse\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS bse_metadata\")\n",
    "        con.execute(\"CREATE TABLE bse_metadata AS SELECT * FROM df_bse_meta\")\n",
    "\n",
    "        # Nifty/BSE indices\n",
    "        con.execute(\"DROP TABLE IF EXISTS All_Nifty_Indices_Meta_Data\")\n",
    "        con.execute(\"CREATE TABLE All_Nifty_Indices_Meta_Data AS SELECT * FROM df_nifty\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS All_BSE_Indices_Meta_Data\")\n",
    "        con.execute(\"CREATE TABLE All_BSE_Indices_Meta_Data AS SELECT * FROM df_bse_indices\")\n",
    "\n",
    "        # Print imported row counts in terminal (including virtuals here)\n",
    "        all_tables = con.execute(\"SHOW TABLES\").fetchdf()['name'].tolist()\n",
    "        print(\"\\nTables currently in DB:\", all_tables)\n",
    "        print(\"\\nImported row counts:\")\n",
    "        for tbl in all_tables:\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {tbl}\").fetchone()[0]\n",
    "                print(f\"{tbl}: {count}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{tbl}: Error getting row count ({e})\")\n",
    "\n",
    "        # --- Create NSE_Master view (FULL OUTER JOIN) ---\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW NSE_Master AS\n",
    "            SELECT a.*, b.*\n",
    "            FROM All_Nifty_Indices_Meta_Data a\n",
    "            FULL OUTER JOIN nse_symbols b\n",
    "            ON a.\"Symbol\" = b.\"SYMBOL\"\n",
    "        \"\"\")\n",
    "        print(\"\\nâœ… NSE_Master view created (join of All_Nifty_Indices_Meta_Data and nse_symbols).\")\n",
    "\n",
    "        # --- Create NSE_Master_Cleaned view (NO EXPORT) ---\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW NSE_Master_Cleaned AS\n",
    "            SELECT\n",
    "                COALESCE(\"Symbol\", SYMBOL_1)                    AS SYMBOL,\n",
    "                COALESCE(\"Company Name\", NAME_OF_COMPANY)        AS COMPANY_NAME,\n",
    "                Industry                                         AS INDUSTRY,\n",
    "                IndexName                                        AS INDEX_NAME\n",
    "            FROM NSE_Master\n",
    "        \"\"\")\n",
    "        print(\"âœ… NSE_Master_Cleaned view created.\")\n",
    "\n",
    "        # --- Create BSE_Master view ---\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW BSE_Master AS\n",
    "            SELECT idx.*, sym.*, meta.*\n",
    "            FROM All_BSE_Indices_Meta_Data idx\n",
    "            FULL OUTER JOIN bse_symbols sym\n",
    "                ON CAST(idx.\"Scrip Code\" AS VARCHAR) = CAST(sym.SECURITY_CODE AS VARCHAR)\n",
    "            FULL OUTER JOIN bse_metadata meta\n",
    "                ON CAST(idx.\"Scrip Code\" AS VARCHAR) = CAST(meta.SC_CODE AS VARCHAR)\n",
    "        \"\"\")\n",
    "        print(\"âœ… BSE_Master view created/joined.\")\n",
    "\n",
    "        # --- Create Cleaned BSE Master view for export ---\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW BSE_Master_Cleaned AS\n",
    "            SELECT\n",
    "                COALESCE(\"Scrip Code\", SC_CODE, SECURITY_CODE) AS SYMBOL_CODE,\n",
    "                COALESCE(COMPANY, ISSUER_NAME) AS COMPANY_NAME,\n",
    "                COALESCE(SECURITY_ID, SC_NAME) AS SYMBOL_NAME,\n",
    "                INDEX_NAME,\n",
    "                SECTOR_NAME,\n",
    "                COALESCE(INDUSTRY_NEW_NAME, INDUSTRY_1) AS INDUSTRY,\n",
    "                INSTRUMENT\n",
    "            FROM BSE_Master\n",
    "        \"\"\")\n",
    "        print(\"âœ… BSE_Master_Cleaned view created.\")\n",
    "\n",
    "        # --- Robust BSE Mapping using Cleaned BSE_Master view ---\n",
    "        def get_bse_symbol_map():\n",
    "            bse_map = {}\n",
    "            # Use SYMBOL_CODE and COMPANY_NAME from BSE_Master_Cleaned\n",
    "            map_df = con.execute(\"\"\"\n",
    "                SELECT SYMBOL_CODE, COMPANY_NAME FROM BSE_Master_Cleaned\n",
    "                WHERE SYMBOL_CODE IS NOT NULL AND COMPANY_NAME IS NOT NULL\n",
    "            \"\"\").fetchdf()\n",
    "            for _, row in map_df.iterrows():\n",
    "                code = str(row['SYMBOL_CODE']).strip().upper()\n",
    "                name = str(row['COMPANY_NAME']).strip()\n",
    "                bse_map[code] = name\n",
    "            return bse_map\n",
    "\n",
    "        # --- Prepare summary for Excel (skip virtuals) ---\n",
    "        skip_virtuals = {'NSE_Master', 'NSE_Master_Cleaned', 'BSE_Master', 'BSE_Master_Cleaned'}\n",
    "        summary = []\n",
    "        table_sources = {\n",
    "            'nse_symbols': nse_loaded_count,\n",
    "            'bse_symbols': bse_loaded_count,\n",
    "            'bse_metadata': bsemeta_loaded_count,\n",
    "            'All_Nifty_Indices_Meta_Data': nifty_count,\n",
    "            'All_BSE_Indices_Meta_Data': bse_indices_count\n",
    "        }\n",
    "\n",
    "        for tbl in all_tables:\n",
    "            if tbl in skip_virtuals:\n",
    "                continue\n",
    "            try:\n",
    "                loaded = table_sources.get(tbl, 'N/A')\n",
    "                written = con.execute(f\"SELECT COUNT(*) FROM {tbl}\").fetchone()[0]\n",
    "                total_rows = written  # Individual table total rows\n",
    "                summary.append([tbl, loaded, written, total_rows, today_str])\n",
    "            except Exception as e:\n",
    "                summary.append([tbl, loaded, f\"Error: {e}\", \"Error\", today_str])\n",
    "\n",
    "    # --- Write to Excel if triggered ---\n",
    "    if write_excel:\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "            sht = wb.sheets[\"Update Dash board\"]\n",
    "            sht.range(\"H2\").value = [[\"Table\", \"Loaded Count\", \"Written Count\", \"Total Rows\", \"Updated Date\"]] + summary\n",
    "        except Exception as e:\n",
    "            print(\"\\n[Excel output skipped: not run from Excel or sheet missing.]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For standalone Python/terminal runs, skip Excel output:\n",
    "    update_symbol_db(write_excel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebffc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import xlwings as xw\n",
    "\n",
    "# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SCRIPT_DIR   = Path(__file__).parent\n",
    "DB_PATH      = SCRIPT_DIR / \"stocks.duckdb\"\n",
    "START_DATE   = date(2019, 1, 1)\n",
    "YESTERDAY    = date.today() - timedelta(days=1)\n",
    "MAX_WORKERS  = 20\n",
    "\n",
    "EXCEL_PATH   = SCRIPT_DIR.parent / \"rubikview.xlsm\"  # adjust if needed\n",
    "EXCEL_SHEET  = \"Update Dash board\"\n",
    "EXCEL_RANGE  = \"H11\"\n",
    "\n",
    "# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def normalize(col: str) -> str:\n",
    "    return col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def get_table_columns(conn):\n",
    "    rows = conn.execute(\"PRAGMA table_info('yahoo_ohlcv')\").fetchall()\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def init_db():\n",
    "    conn = duckdb.connect(str(DB_PATH))\n",
    "    conn.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS yahoo_ohlcv (\n",
    "        symbol VARCHAR,\n",
    "        date   DATE\n",
    "      );\n",
    "    \"\"\")\n",
    "    conn.close()\n",
    "\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "def load_symbols():\n",
    "    SYMBOLS_DB = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\")\n",
    "    conn = duckdb.connect(str(SYMBOLS_DB))\n",
    "\n",
    "    # Fetch cleaned symbols for NSE and BSE from the cleaned master table\n",
    "    nse = conn.execute(\"SELECT DISTINCT SYMBOL FROM NSE_Master_Cleaned WHERE SYMBOL IS NOT NULL\").fetchdf()\n",
    "    nse_syms = (nse['SYMBOL'].astype(str).str.strip() + \".NS\").drop_duplicates()\n",
    "\n",
    "    bse = conn.execute(\"SELECT DISTINCT SYMBOL_NAME FROM BSE_Master_Cleaned WHERE SYMBOL_NAME IS NOT NULL\").fetchdf()\n",
    "    bse_syms = (bse['SYMBOL_NAME'].astype(str).str.strip() + \".BO\").drop_duplicates()\n",
    "\n",
    "\n",
    "    all_syms = pd.concat([nse_syms, bse_syms]).dropna().unique().tolist()\n",
    "    conn.close()\n",
    "    return all_syms\n",
    "\n",
    "\n",
    "def insert_dynamic(conn, df: pd.DataFrame):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    df = df.rename(columns=normalize)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['symbol'] = df['symbol']\n",
    "    existing = get_table_columns(conn)\n",
    "\n",
    "    # --- Ensure all columns in df are present in table, add if not ---\n",
    "    for col in df.columns:\n",
    "        if col not in existing:\n",
    "            # Use DOUBLE for all numeric columns except symbol and date\n",
    "            if col in ('symbol', 'date'):\n",
    "                continue\n",
    "            conn.execute(f'ALTER TABLE yahoo_ohlcv ADD COLUMN {col} DOUBLE;')\n",
    "            existing.append(col)\n",
    "\n",
    "    # --- Align df columns with table columns, fill missing with None ---\n",
    "    for col in existing:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    to_ins = df[existing]  # ensure column order matches table\n",
    "\n",
    "    conn.register(\"new_data\", to_ins)\n",
    "    conn.execute(f\"INSERT INTO yahoo_ohlcv SELECT * FROM new_data;\")\n",
    "    conn.unregister(\"new_data\")\n",
    "\n",
    "\n",
    "def fetch_and_insert(symbol):\n",
    "    fetch_date = YESTERDAY\n",
    "    fetch_end = YESTERDAY + timedelta(days=1)\n",
    "\n",
    "    # Fetch the latest value from yfinance\n",
    "    df = yf.Ticker(symbol).history(\n",
    "        start=fetch_date,\n",
    "        end=fetch_end,\n",
    "        auto_adjust=False,\n",
    "        actions=True\n",
    "    )\n",
    "    if df.empty:\n",
    "        return symbol, 0, None, None, \"skipped\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    # Fetch the current value from DB for this symbol and date\n",
    "    conn_w = duckdb.connect(str(DB_PATH))\n",
    "    db_row = conn_w.execute(\n",
    "        \"SELECT * FROM yahoo_ohlcv WHERE symbol=? AND date=?\",\n",
    "        (symbol, fetch_date)\n",
    "    ).fetchdf()\n",
    "\n",
    "    # Normalize columns for comparison\n",
    "    df_to_compare = df.copy()\n",
    "    df_to_compare.columns = [col.lower() for col in df_to_compare.columns]\n",
    "    db_row.columns = [col.lower() for col in db_row.columns]\n",
    "\n",
    "    # Check for difference\n",
    "    values_are_different = True\n",
    "    if not db_row.empty:\n",
    "        db_row_sorted = db_row[df_to_compare.columns.intersection(db_row.columns)]\n",
    "        df_row_sorted = df_to_compare[df_to_compare.columns.intersection(db_row.columns)]\n",
    "        values_are_different = not db_row_sorted.equals(df_row_sorted)\n",
    "\n",
    "    if not values_are_different:\n",
    "        conn_w.close()\n",
    "        return symbol, 0, None, None, \"up to Date\"  # Skip if identical\n",
    "\n",
    "    # Delete and insert if different\n",
    "    min_date = df['Date'].min().date()\n",
    "    max_date = df['Date'].max().date()\n",
    "    conn_w.execute(\n",
    "        \"DELETE FROM yahoo_ohlcv WHERE symbol=? AND date BETWEEN ? AND ?\",\n",
    "        (symbol, min_date, max_date)\n",
    "    )\n",
    "    insert_dynamic(conn_w, df)\n",
    "    conn_w.close()\n",
    "\n",
    "    return symbol, len(df), min_date, max_date, \"success\"\n",
    "\n",
    "def get_ohlcv_table_name(db_path):\n",
    "    with duckdb.connect(str(db_path), read_only=True) as con:\n",
    "        tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "        for t in tables.iloc[:, 0]:  # Table names column\n",
    "            cols = con.execute(f\"PRAGMA table_info('{t}')\").fetchdf()['name'].str.lower().tolist()\n",
    "            if 'symbol' in cols and 'date' in cols:\n",
    "                return t\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "def write_progress_to_excel(done, total, progress_count, success, failed, skipped, uptodate, processed, updatedate):\n",
    "    try:\n",
    "        app = xw.apps.active if xw.apps else xw.App(visible=True)\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "        except Exception:\n",
    "            wb = xw.Book(EXCEL_PATH)\n",
    "        sht = wb.sheets[EXCEL_SHEET]\n",
    "\n",
    "        headers = [\n",
    "            \"Table\", \"Progress %\", \"Progress Count\", \"Success\", \"Failed\", \"Skipped\",\n",
    "            \"Up to Date\", \"Processed\", \"Update Date\"\n",
    "        ]\n",
    "        progress_num = success + failed + skipped + uptodate\n",
    "\n",
    "        # --- Dynamically detect OHLCV table name ---\n",
    "        ohlcv_table = get_ohlcv_table_name(DB_PATH)\n",
    "        table_display = f\"{ohlcv_table} ({DB_PATH.name})\"\n",
    "\n",
    "        row = [\n",
    "            f\"{ohlcv_table} ({DB_PATH.name})\",\n",
    "            f\"{(progress_num / total * 100):.2f}%\" if total else \"N/A\",\n",
    "            f\"{done}/{total}\",\n",
    "            success,\n",
    "            failed,\n",
    "            skipped,\n",
    "            uptodate,\n",
    "            processed,\n",
    "            updatedate\n",
    "        ]\n",
    "        sht.range(\"H11\").value = headers\n",
    "        sht.range(\"H12\").value = row\n",
    "    except Exception as e:\n",
    "        print(f\"Excel output error: {e}\")\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    init_db()\n",
    "    symbols = load_symbols()\n",
    "    total = len(symbols)\n",
    "    today_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Trackers\n",
    "    success = failed = skipped = uptodate = 0\n",
    "    processed = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "        futures = {exe.submit(fetch_and_insert, s): s for s in symbols}\n",
    "        for idx, future in enumerate(as_completed(futures), start=1):\n",
    "            sym = futures[future]\n",
    "            percent = idx / total * 100\n",
    "            try:\n",
    "                _, count, first_dt, last_dt, status_flag = future.result()\n",
    "                processed += 1\n",
    "                if status_flag == \"success\":\n",
    "                    success += 1\n",
    "                    status = f\"+{count} rows ({first_dt}â†’{last_dt})\"\n",
    "                elif status_flag == \"skipped\":\n",
    "                    skipped += 1\n",
    "                    status = \"skipped\"\n",
    "                elif status_flag == \"uptodate\":\n",
    "                    uptodate += 1\n",
    "                    status = \"up-to-date\"\n",
    "                else:\n",
    "                    status = status_flag\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                status = f\"FAILED: {e}\"\n",
    "\n",
    "            print(f\"{idx}/{total} ({percent:.1f}%): {sym} â†’ {status}\")\n",
    "\n",
    "            # --- Excel output: Only update Excel every 10th step or last row\n",
    "            if idx % 10 == 0 or idx == total:\n",
    "                write_progress_to_excel(\n",
    "                    idx, total, f\"{idx}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "                )\n",
    "\n",
    "    # Final write\n",
    "    write_progress_to_excel(\n",
    "        total, total, f\"{total}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "    )\n",
    "    print(\"\\nâœ… Ultra-fast parallel update complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For command line testing, uncomment:\n",
    "    # xw.Book(EXCEL_PATH).set_mock_caller()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3310ddc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3502505360.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def write_progress_to_excel(...):\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def write_progress_to_excel(...):\n",
    "    try:\n",
    "        print(\"Trying to write to Excel...\")\n",
    "        if not os.path.exists(EXCEL_PATH):\n",
    "            print(f\"Excel output error: File {EXCEL_PATH} not found.\")\n",
    "            return\n",
    "        app = xw.apps.active if xw.apps else xw.App(visible=True)\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "        except Exception:\n",
    "            wb = xw.Book(EXCEL_PATH)\n",
    "        print(\"Workbook opened.\")\n",
    "        sht = wb.sheets[EXCEL_SHEET]\n",
    "        print(f\"Writing to sheet: {EXCEL_SHEET}\")\n",
    "        # ... rest of the code ...\n",
    "        sht.range(\"H11\").value = headers\n",
    "        sht.range(\"H12\").value = row\n",
    "        print(\"Write to Excel completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Excel output error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import xlwings as xw\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SCRIPT_DIR   = Path(__file__).parent\n",
    "DB_PATH      = SCRIPT_DIR / \"stocks.duckdb\"\n",
    "YEARS_BACK   = 6\n",
    "START_DATE   = date.today() - timedelta(days=YEARS_BACK * 365)\n",
    "YESTERDAY    = date.today() - timedelta(days=1)\n",
    "MAX_WORKERS  = 20\n",
    "\n",
    "EXCEL_PATH   = SCRIPT_DIR.parent / \"rubikview.xlsm\"\n",
    "EXCEL_SHEET  = \"Update Dash board\"\n",
    "EXCEL_RANGE  = \"H11\"\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def normalize(col: str) -> str:\n",
    "    return col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def get_table_columns(conn):\n",
    "    rows = conn.execute(\"PRAGMA table_info('yahoo_ohlcv')\").fetchall()\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def init_db():\n",
    "    conn = duckdb.connect(str(DB_PATH))\n",
    "    conn.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS yahoo_ohlcv (\n",
    "        symbol VARCHAR,\n",
    "        date   DATE\n",
    "      );\n",
    "    \"\"\")\n",
    "    conn.close()\n",
    "\n",
    "def load_symbols():\n",
    "    SYMBOLS_DB = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\")\n",
    "    conn = duckdb.connect(str(SYMBOLS_DB))\n",
    "    # Fetch cleaned symbols for NSE and BSE from the cleaned master table\n",
    "    nse = conn.execute(\"SELECT DISTINCT SYMBOL FROM NSE_Master_Cleaned WHERE SYMBOL IS NOT NULL\").fetchdf()\n",
    "    nse_syms = (nse['SYMBOL'].astype(str).str.strip() + \".NS\").drop_duplicates()\n",
    "    bse = conn.execute(\"SELECT DISTINCT SYMBOL_NAME FROM BSE_Master_Cleaned WHERE SYMBOL_NAME IS NOT NULL\").fetchdf()\n",
    "    bse_syms = (bse['SYMBOL_NAME'].astype(str).str.strip() + \".BO\").drop_duplicates()\n",
    "    all_syms = pd.concat([nse_syms, bse_syms]).dropna().unique().tolist()\n",
    "    conn.close()\n",
    "    return all_syms\n",
    "\n",
    "def insert_dynamic(conn, df: pd.DataFrame):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    df = df.rename(columns=normalize)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['symbol'] = df['symbol']\n",
    "    existing = get_table_columns(conn)\n",
    "    # Ensure all columns in df are present in table, add if not\n",
    "    for col in df.columns:\n",
    "        if col not in existing:\n",
    "            if col in ('symbol', 'date'):\n",
    "                continue\n",
    "            conn.execute(f'ALTER TABLE yahoo_ohlcv ADD COLUMN {col} DOUBLE;')\n",
    "            existing.append(col)\n",
    "    for col in existing:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    to_ins = df[existing]\n",
    "    conn.register(\"new_data\", to_ins)\n",
    "    conn.execute(f\"INSERT INTO yahoo_ohlcv SELECT * FROM new_data;\")\n",
    "    conn.unregister(\"new_data\")\n",
    "\n",
    "def fetch_and_insert(symbol):\n",
    "    # Always fetch from START_DATE (6 years ago) to YESTERDAY\n",
    "    fetch_start = START_DATE\n",
    "    fetch_end = YESTERDAY + timedelta(days=1)\n",
    "    conn_r = duckdb.connect(str(DB_PATH))\n",
    "    # Find latest date present in DB for symbol, if any\n",
    "    last = conn_r.execute(\"SELECT MAX(date) FROM yahoo_ohlcv WHERE symbol=?\", (symbol,)).fetchone()[0]\n",
    "    conn_r.close()\n",
    "    # Only fetch missing dates (after latest date)\n",
    "    if last:\n",
    "        last_dt = pd.to_datetime(last).date()\n",
    "        if last_dt >= YESTERDAY:\n",
    "            return symbol, 0, None, None, \"uptodate\"\n",
    "        fetch_start = max(fetch_start, last_dt + timedelta(days=1))\n",
    "    if fetch_start > YESTERDAY:\n",
    "        return symbol, 0, None, None, \"uptodate\"\n",
    "    df = yf.Ticker(symbol).history(\n",
    "        start=fetch_start,\n",
    "        end=fetch_end,\n",
    "        auto_adjust=False,\n",
    "        actions=True\n",
    "    )\n",
    "    if df.empty:\n",
    "        return symbol, 0, None, None, \"skipped\"\n",
    "    df = df.reset_index()\n",
    "    df['symbol'] = symbol\n",
    "    conn_w = duckdb.connect(str(DB_PATH))\n",
    "    # For each row, if already present for (symbol, date), check if values match\n",
    "    for _, row in df.iterrows():\n",
    "        row_date = pd.to_datetime(row['Date']).date()\n",
    "        db_row = conn_w.execute(\n",
    "            \"SELECT * FROM yahoo_ohlcv WHERE symbol=? AND date=?\",\n",
    "            (symbol, row_date)\n",
    "        ).fetchdf()\n",
    "        # Normalize columns for comparison\n",
    "        values_are_different = True\n",
    "        if not db_row.empty:\n",
    "            db_row_sorted = db_row[df.columns.intersection(db_row.columns)]\n",
    "            df_row_sorted = pd.DataFrame([row])[db_row.columns.intersection(df.columns)]\n",
    "            values_are_different = not db_row_sorted.equals(df_row_sorted)\n",
    "        if values_are_different:\n",
    "            # Remove any old for that date and insert new\n",
    "            conn_w.execute(\"DELETE FROM yahoo_ohlcv WHERE symbol=? AND date=?\", (symbol, row_date))\n",
    "            insert_dynamic(conn_w, pd.DataFrame([row]))\n",
    "    conn_w.close()\n",
    "    first_dt = df['Date'].min().date()\n",
    "    last_dt  = df['Date'].max().date()\n",
    "    return symbol, len(df), first_dt, last_dt, \"success\"\n",
    "\n",
    "def get_ohlcv_table_name(db_path):\n",
    "    with duckdb.connect(str(db_path), read_only=True) as con:\n",
    "        tables = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "        for t in tables.iloc[:, 0]:  # Table names column\n",
    "            cols = con.execute(f\"PRAGMA table_info('{t}')\").fetchdf()['name'].str.lower().tolist()\n",
    "            if 'symbol' in cols and 'date' in cols:\n",
    "                return t\n",
    "    return \"N/A\"\n",
    "\n",
    "def write_progress_to_excel(done, total, progress_count, success, failed, skipped, uptodate, processed, updatedate):\n",
    "    try:\n",
    "        if not os.path.exists(EXCEL_PATH):\n",
    "            print(f\"Excel output error: File {EXCEL_PATH} not found.\")\n",
    "            return\n",
    "        app = xw.apps.active if xw.apps else xw.App(visible=True)\n",
    "        try:\n",
    "            wb = xw.Book.caller()\n",
    "        except Exception:\n",
    "            wb = xw.Book(EXCEL_PATH)\n",
    "        sht = wb.sheets[EXCEL_SHEET]\n",
    "        headers = [\n",
    "            \"Table\", \"Progress %\", \"Progress Count\", \"Success\", \"Failed\", \"Skipped\",\n",
    "            \"Up to Date\", \"Processed\", \"Update Date\"\n",
    "        ]\n",
    "        progress_num = success + failed + skipped + uptodate\n",
    "        ohlcv_table = get_ohlcv_table_name(DB_PATH)\n",
    "        row = [\n",
    "            f\"{ohlcv_table} ({DB_PATH.name})\",\n",
    "            f\"{(progress_num / total * 100):.2f}%\" if total else \"N/A\",\n",
    "            f\"{done}/{total}\",\n",
    "            success,\n",
    "            failed,\n",
    "            skipped,\n",
    "            uptodate,\n",
    "            processed,\n",
    "            updatedate\n",
    "        ]\n",
    "        sht.range(\"H11\").value = headers\n",
    "        sht.range(\"H12\").value = row\n",
    "    except Exception as e:\n",
    "        print(f\"Excel output error: {e}\")\n",
    "\n",
    "# ===== MAIN =====\n",
    "def main():\n",
    "    init_db()\n",
    "    symbols = load_symbols()\n",
    "    total = len(symbols)\n",
    "    today_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    success = failed = skipped = uptodate = 0\n",
    "    processed = 0\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "        futures = {exe.submit(fetch_and_insert, s): s for s in symbols}\n",
    "        for idx, future in enumerate(as_completed(futures), start=1):\n",
    "            sym = futures[future]\n",
    "            percent = idx / total * 100\n",
    "            try:\n",
    "                _, count, first_dt, last_dt, status_flag = future.result()\n",
    "                processed += 1\n",
    "                if status_flag == \"success\":\n",
    "                    success += 1\n",
    "                    status = f\"+{count} rows ({first_dt}â†’{last_dt})\"\n",
    "                elif status_flag == \"skipped\":\n",
    "                    skipped += 1\n",
    "                    status = \"skipped\"\n",
    "                elif status_flag == \"uptodate\":\n",
    "                    uptodate += 1\n",
    "                    status = \"up-to-date\"\n",
    "                else:\n",
    "                    status = status_flag\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                status = f\"FAILED: {e}\"\n",
    "            print(f\"{idx}/{total} ({percent:.1f}%): {sym} â†’ {status}\")\n",
    "            if idx % 10 == 0 or idx == total:\n",
    "                write_progress_to_excel(\n",
    "                    idx, total, f\"{idx}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "                )\n",
    "    write_progress_to_excel(\n",
    "        total, total, f\"{total}/{total}\", success, failed, skipped, uptodate, processed, today_str\n",
    "    )\n",
    "    print(\"\\nâœ… Ultra-fast parallel update complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd763c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from indicators import *\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "# ===== PATHS =====\n",
    "ROOT = Path(r\"C:/Users/jallu/OneDrive/pgp/Python/Stock predictor/Rubik_view\")\n",
    "EXCEL_FILE = ROOT / \"rubikview.xlsm\"\n",
    "INDICATOR_SHEET = \"Indicators\"\n",
    "OHLCV_DB = ROOT / \"Data\" / \"OHCLV Data\" / \"stocks.duckdb\"\n",
    "SIGNALS_DB = ROOT / \"Data\" / \"Signals Data\" / \"signals.duckdb\"\n",
    "DASH_SHEET = \"Update Dash board\"\n",
    "PROGRESS_CELL = \"K18\"\n",
    "\n",
    "progress_lock = threading.Lock()\n",
    "\n",
    "def update_excel_summary(done, total, messages, dash_sheet):\n",
    "    success_count = sum(1 for m in messages if \"processed\" in m)\n",
    "    fail_count = sum(1 for m in messages if \"error\" in m)\n",
    "    skipped_count = sum(1 for m in messages if \"skipped\" in m)\n",
    "    uptodate_count = sum(1 for m in messages if \"up-to-date\" in m)\n",
    "    processed_symbols = success_count + fail_count + skipped_count + uptodate_count\n",
    "\n",
    "    progress_str = (\n",
    "        f\"{processed_symbols / total * 100:.2f}%\"\n",
    "        if total else \"N/A\"\n",
    "    )\n",
    "    progress_count_str = f\"{done}/{total}\"\n",
    "\n",
    "    with duckdb.connect(str(SIGNALS_DB), read_only=True) as con:\n",
    "        table_list = con.execute(\"SHOW TABLES\").fetchdf()['name'].tolist()\n",
    "        table_name = ', '.join(table_list) if table_list else \"N/A\"\n",
    "\n",
    "    headers = [\n",
    "        \"Table\", \"Progress %\", \"Progress Count\", \"Success\", \"Failed\", \"Skipped\",\n",
    "        \"Up to Date\", \"Processed\", \"Update Date\"\n",
    "    ]\n",
    "    row = [\n",
    "        f\"{table_name} ({SIGNALS_DB.name})\",\n",
    "        progress_str,\n",
    "        progress_count_str,\n",
    "        success_count,\n",
    "        fail_count,\n",
    "        skipped_count,\n",
    "        uptodate_count,\n",
    "        processed_symbols,\n",
    "        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ]\n",
    "    dash_sheet.range(\"H18\").value = headers\n",
    "    dash_sheet.range(\"H19\").value = row\n",
    "\n",
    "def update_excel_progress(done, total, messages):\n",
    "    try:\n",
    "        with progress_lock:\n",
    "            book = xw.Book(str(EXCEL_FILE))\n",
    "            sht = book.sheets[DASH_SHEET]\n",
    "            success_count = sum(1 for m in messages if \"processed\" in m)\n",
    "            current_value = sht.range(PROGRESS_CELL).value\n",
    "            # Only update K18 if changed, prevents flicker\n",
    "            if success_count > 0 and current_value != success_count:\n",
    "                # Uncomment the next line for debug output:\n",
    "                # print(f\"Updating K18 from {current_value} to {success_count}\")\n",
    "                sht.range(PROGRESS_CELL).value = success_count\n",
    "            # Always update the summary table\n",
    "            update_excel_summary(done, total, messages, sht)\n",
    "    except Exception as e:\n",
    "        print(f\"Excel update error: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Optionally clear dashboard summary area at start (DO NOT clear K18)\n",
    "    try:\n",
    "        book = xw.Book(str(EXCEL_FILE))\n",
    "        sht = book.sheets[DASH_SHEET]\n",
    "        # Do NOT clear K18 at start, only the summary area\n",
    "        sht.range(\"H18:P25\").value = \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Read indicator config from Excel\n",
    "    try:\n",
    "        sht = xw.Book(str(EXCEL_FILE)).sheets[INDICATOR_SHEET]\n",
    "        ind_df = sht.range(\"A1\").options(pd.DataFrame, header=1, index=False, expand='table').value\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not read Excel config: {e}\")\n",
    "        raise SystemExit\n",
    "    active_inds = ind_df[ind_df['Active'].str.upper() == 'Y'].copy()\n",
    "    if active_inds.empty:\n",
    "        print(\"No active indicators! Exiting.\")\n",
    "        exit(1)\n",
    "    print(f\"Active indicators: {len(active_inds)}\")\n",
    "\n",
    "    # Get symbols from OHLCV DB\n",
    "    with duckdb.connect(str(OHLCV_DB), read_only=True) as con:\n",
    "        symbols = con.execute(\"SELECT DISTINCT symbol FROM yahoo_ohlcv\").fetchdf()['symbol'].tolist()\n",
    "    print(f\"Symbols: {len(symbols)}\")\n",
    "\n",
    "    # Prep signals DB\n",
    "    with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS signals (\n",
    "                symbol VARCHAR,\n",
    "                date DATE\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # Indicator calculation map (unchanged)\n",
    "    def calculate_indicator(name, df, p1, p2, p3):\n",
    "        # [Same as before, your indicator switch...]\n",
    "        if name == \"RSI\":\n",
    "            return calculate_rsi(df['Close'], int(p1))\n",
    "        elif name == \"MFI\":\n",
    "            return calculate_mfi(df['High'], df['Low'], df['Close'], df['Volume'], int(p1))\n",
    "        elif name == \"CCI\":\n",
    "            return calculate_cci(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"StochRSI\":\n",
    "            return calculate_stochrsi(df['Close'], int(p1), int(p2), int(p3))\n",
    "        elif name == \"ROC\":\n",
    "            return calculate_roc(df['Close'], int(p1))\n",
    "        elif name == \"MACD\":\n",
    "            return calculate_macd(df['Close'], int(p1), int(p2), int(p3))\n",
    "        elif name == \"EMA Crossover\":\n",
    "            return calculate_ema_crossover(df['Close'], int(p1), int(p2))\n",
    "        elif name == \"SMA Crossover\":\n",
    "            return calculate_sma_crossover(df['Close'], int(p1), int(p2))\n",
    "        elif name == \"ATR\":\n",
    "            return calculate_atr(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"Williams %R\":\n",
    "            return calculate_williams_r(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"ADX\":\n",
    "            return calculate_adx(df['High'], df['Low'], df['Close'], int(p1))\n",
    "        elif name == \"VWAP\":\n",
    "            return calculate_vwap(df)\n",
    "        elif name == \"SuperTrend\":\n",
    "            return calculate_supertrend(df, int(p1), float(p2))\n",
    "        elif name == \"Parabolic SAR\":\n",
    "            return calculate_parabolic_sar(df['High'], df['Low'], float(p1), float(p2), float(p3))\n",
    "        elif name == \"Ichimoku\":\n",
    "            return calculate_ichimoku(df, int(p1), int(p2), int(p3))\n",
    "        elif name == \"Bollinger Bands\":\n",
    "            return calculate_bollinger(df['Close'], int(p1), float(p2))\n",
    "        elif name == \"Donchian Channel\":\n",
    "            return calculate_donchian(df['High'], df['Low'], int(p1))\n",
    "        elif name == \"Keltner Channel\":\n",
    "            return calculate_keltner(df, int(p1))\n",
    "        elif name == \"VMA\":\n",
    "            return calculate_vma(df['Volume'], int(p1)) \n",
    "        elif name == \"OBV\":\n",
    "            return calculate_obv(df['Close'], df['Volume'])\n",
    "        elif name == \"ADL\":\n",
    "            return calculate_adl(duckdb.df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # Main per-symbol worker\n",
    "    def process_symbol(sym):\n",
    "        try:\n",
    "            with duckdb.connect(str(OHLCV_DB), read_only=True) as con:\n",
    "                df = con.execute(\n",
    "                    \"SELECT date, open, high, low, close, adj_close, volume FROM yahoo_ohlcv WHERE symbol=? ORDER BY date\",\n",
    "                    (sym,)\n",
    "                ).fetchdf()\n",
    "            if df.empty:\n",
    "                return None, f\"{sym} | skipped (no data)\"\n",
    "            last_date = df['date'].iloc[-1]\n",
    "            with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "                already = con.execute(\"SELECT COUNT(*) FROM signals WHERE symbol=? AND date=?\", (sym, last_date)).fetchone()[0]\n",
    "            if already:\n",
    "                return None, f\"{sym} | up-to-date\"\n",
    "            df = df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'})\n",
    "            row = {'symbol': sym, 'date': last_date}\n",
    "            for _, ind in active_inds.iterrows():\n",
    "                name = ind['Indicator_Name']\n",
    "                p1 = ind['Parameter_1'] if not pd.isnull(ind['Parameter_1']) else None\n",
    "                p2 = ind['Parameter_2'] if not pd.isnull(ind['Parameter_2']) else None\n",
    "                p3 = ind['Parameter_3'] if not pd.isnull(ind['Parameter_3']) else None\n",
    "                mweight = float(ind['Manual_Weight']) if not pd.isnull(ind['Manual_Weight']) else 1.0\n",
    "                use_ai  = str(ind['Use_AI_Weight']).upper() == \"Y\"\n",
    "                aiweight = float(ind['AI_Latest_Weight']) if not pd.isnull(ind['AI_Latest_Weight']) else mweight\n",
    "                weight = aiweight if use_ai else mweight\n",
    "                try:\n",
    "                    signal = calculate_indicator(name, df, p1, p2, p3)\n",
    "                except Exception as ee:\n",
    "                    signal = np.nan\n",
    "                icol = name.lower().replace(\" \", \"_\").replace(\"%\", \"pct\")\n",
    "                row[f\"{icol}_signal\"] = signal   # store as float, not int!\n",
    "                row[f\"{icol}_weight\"] = weight\n",
    "            return row, f\"{sym} | processed\"\n",
    "        except Exception as e:\n",
    "            return None, f\"{sym} | error: {str(e)}\"\n",
    "\n",
    "    # FAST PARALLEL EXECUTION\n",
    "    results = []\n",
    "    messages = []\n",
    "    N_THREADS = 20\n",
    "    UPDATE_FREQ = 10  # update summary in Excel every 10 symbols\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_THREADS) as executor:\n",
    "        futures = {executor.submit(process_symbol, sym): sym for sym in symbols}\n",
    "        done = 0\n",
    "        for fut in as_completed(futures):\n",
    "            row, msg = fut.result()\n",
    "            done += 1\n",
    "            print(f\"[{done}/{len(symbols)}] {msg}\")\n",
    "            if row is not None:\n",
    "                results.append(row)\n",
    "            messages.append(msg)\n",
    "            if done % UPDATE_FREQ == 0 or done == len(symbols):\n",
    "                update_excel_progress(done, len(symbols), messages)\n",
    "\n",
    "    print(\"âœ… Done.\")\n",
    "    update_excel_progress(len(symbols), len(symbols), messages)   # Final update\n",
    "\n",
    "    # FORCE EXCEL RECALC (optional, helps show correct numbers immediately)\n",
    "    try:\n",
    "        book = xw.Book(str(EXCEL_FILE))\n",
    "        sht = book.sheets[DASH_SHEET]\n",
    "        sht.api.Calculate()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # INSERT TO SIGNALS DB\n",
    "    if results:\n",
    "        df_signals = pd.DataFrame(results)\n",
    "        with duckdb.connect(str(SIGNALS_DB)) as con:\n",
    "            dbcols = [c[1] for c in con.execute(\"PRAGMA table_info('signals')\").fetchall()]\n",
    "            for c in df_signals.columns:\n",
    "                if c not in dbcols:\n",
    "                    if c.endswith(\"_weight\"):\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} DOUBLE\")\n",
    "                    elif c.endswith(\"_signal\"):\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} DOUBLE\")\n",
    "                    else:\n",
    "                        con.execute(f\"ALTER TABLE signals ADD COLUMN {c} VARCHAR\")\n",
    "            con.register(\"batch\", df_signals)\n",
    "            con.execute(\"INSERT INTO signals SELECT * FROM batch\")\n",
    "            con.unregister(\"batch\")\n",
    "        print(\"ðŸŽ‰ All signals saved to signals.duckdb.\")\n",
    "    else:\n",
    "        print(\"âš  No new signals to insert.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import xlwings as xw\n",
    "import numpy as np\n",
    "\n",
    "def classify_signal(score, min_score, max_score):\n",
    "    if max_score == min_score:\n",
    "        norm_val = 0\n",
    "    else:\n",
    "        norm_val = (score - min_score) / (max_score - min_score) * 20 - 10\n",
    "    if norm_val >= 7:\n",
    "        sig = \"Extreem Bullish â¬†ï¸â¬†ï¸ \"\n",
    "    elif norm_val >= 3:\n",
    "        sig = \"Bullishâ¬†ï¸\"\n",
    "    elif norm_val > -3:\n",
    "        sig = \"Hold â†”ï¸\"\n",
    "    elif norm_val > -7:\n",
    "        sig = \"Bearish â¬‡ï¸\"\n",
    "    else:\n",
    "        sig = \"Extreem Bearish â¬‡ï¸â¬‡ï¸\"\n",
    "    return round(norm_val, 2), sig\n",
    "\n",
    "# --- Robust NSE Mapping using Master Data ---\n",
    "def get_nse_symbol_map():\n",
    "    SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "    nse_map = {}\n",
    "    with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "        nse_df = con.execute(\"\"\"\n",
    "            SELECT SYMBOL, \"COMPANY_NAME\"\n",
    "            FROM NSE_Master_Cleaned\n",
    "            WHERE SYMBOL IS NOT NULL AND \"COMPANY_NAME\" IS NOT NULL\n",
    "        \"\"\").fetchdf()\n",
    "        for _, row in nse_df.iterrows():\n",
    "            symbol = str(row['SYMBOL']).strip().upper()\n",
    "            cname = str(row['COMPANY_NAME']).strip()\n",
    "            nse_map[symbol] = cname\n",
    "    return nse_map\n",
    "\n",
    "# --- Robust BSE Mapping using Master Data ---\n",
    "def get_bse_symbol_map():\n",
    "    SYMBOLS_DB = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\Data\\Symbols Data\\symbols.duckdb\"\n",
    "    bse_map = {}\n",
    "    with duckdb.connect(SYMBOLS_DB, read_only=True) as con:\n",
    "        bse_df = con.execute(\"\"\"\n",
    "            SELECT SYMBOL_NAME, COMPANY_NAME\n",
    "            FROM BSE_Master_Cleaned\n",
    "            WHERE SYMBOL_NAME IS NOT NULL AND COMPANY_NAME IS NOT NULL\n",
    "        \"\"\").fetchdf()\n",
    "        for _, row in bse_df.iterrows():\n",
    "            code = str(row['SYMBOL_NAME']).strip().upper()\n",
    "            cname = str(row['COMPANY_NAME']).strip()\n",
    "            bse_map[code] = cname\n",
    "    return bse_map\n",
    "\n",
    "\n",
    "# --- Universal Symbol Resolver ---\n",
    "def resolve_name(symbol, nse_map, bse_map):\n",
    "    sym = str(symbol).strip().upper()\n",
    "    if sym.endswith('.NS'):\n",
    "        base = sym[:-3]\n",
    "        return nse_map.get(base, \"\") or \"\"\n",
    "    elif sym.endswith('.BO'):\n",
    "        base = sym[:-3]\n",
    "        return bse_map.get(base, \"\") or bse_map.get(sym, \"\") or \"\"\n",
    "    else:\n",
    "        return nse_map.get(sym, \"\") or bse_map.get(sym, \"\") or \"\"\n",
    "\n",
    "def main():\n",
    "    BASE_DIR = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\"\n",
    "    SIGNALS_DB = f\"{BASE_DIR}\\\\Data\\\\Signals Data\\\\signals.duckdb\"\n",
    "    INDICATOR_SHEET = \"Indicators\"\n",
    "    OUTPUT_SHEET = \"Auto Stock Selection\"\n",
    "\n",
    "    wb = xw.Book.caller()\n",
    "    ind_sheet = wb.sheets[INDICATOR_SHEET]\n",
    "    out_sheet = wb.sheets[OUTPUT_SHEET]\n",
    "\n",
    "    out_sheet.range(\"N4\").value = \"Input Count\"\n",
    "    try:\n",
    "        n_picks = int(out_sheet.range(\"O4\").value)\n",
    "        if n_picks <= 0:\n",
    "            n_picks = 10\n",
    "    except:\n",
    "        n_picks = 10\n",
    "\n",
    "    # --- Load only active indicators ---\n",
    "    ind_df = ind_sheet.range(\"A1\").options(pd.DataFrame, header=1, index=False, expand='table').value\n",
    "    ind_df.columns = ind_df.columns.str.strip().str.lower()\n",
    "    col_active = [col for col in ind_df.columns if 'active' in col][0]\n",
    "    col_name = [col for col in ind_df.columns if 'indicator' in col and 'name' in col][0]\n",
    "    col_weight = [col for col in ind_df.columns if 'weight' in col and 'manual' in col][0]\n",
    "    active = ind_df[ind_df[col_active].str.upper() == 'Y']\n",
    "    weights = dict(zip(active[col_name], active[col_weight]))\n",
    "\n",
    "    # --- Indicator count and categories summary ---\n",
    "    out_sheet.range(\"O7\").value = len(active)\n",
    "    out_sheet.range(\"N7\").value = \"Indicators\"\n",
    "    categories = active['category'].dropna().unique()\n",
    "    category_counts = active['category'].value_counts()\n",
    "    for idx, cat in enumerate(categories):\n",
    "        n_row = 8 + idx\n",
    "        out_sheet.range(f\"N{n_row}\").value = cat\n",
    "        out_sheet.range(f\"O{n_row}\").value = int(category_counts[cat])\n",
    "\n",
    "    # --- Fetch latest signals (only for active indicators) ---\n",
    "    with duckdb.connect(SIGNALS_DB, read_only=True) as con:\n",
    "        cols = con.execute(\"PRAGMA table_info('signals')\").fetchdf()\n",
    "        act_sig_cols = [\n",
    "            indicator.lower().replace(\" \", \"_\").replace(\"%\", \"pct\") + \"_signal\"\n",
    "            for indicator in weights\n",
    "        ]\n",
    "        act_sig_cols = [c for c in act_sig_cols if c in cols['name'].values]\n",
    "        select_cols = ['symbol', 'date'] + act_sig_cols\n",
    "        if not act_sig_cols:\n",
    "            out_sheet.range(\"A:E\").clear_contents()\n",
    "            out_sheet.range(\"A1\").options(index=False).value = [[\"Name\", \"Symbol\", \"Score\", \"Signal Range (-10 to 10)\", \"Signal Text\"]]\n",
    "            print(\"No active indicator signals found in DB!\")\n",
    "            return\n",
    "        df = con.execute(f\"SELECT {', '.join(select_cols)} FROM signals\").fetchdf()\n",
    "\n",
    "    df = df.sort_values('date').groupby('symbol').tail(1).reset_index(drop=True)\n",
    "\n",
    "    # --- Compute scores using only active indicator columns ---\n",
    "    score_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        total_score = 0\n",
    "        for indicator, weight in weights.items():\n",
    "            ind_key = indicator.lower().replace(\" \", \"_\").replace(\"%\", \"pct\") + \"_signal\"\n",
    "            signal = row.get(ind_key, None)\n",
    "            if signal is None or pd.isnull(signal):\n",
    "                continue\n",
    "            try:\n",
    "                score = float(signal) * float(weight)\n",
    "                total_score += score\n",
    "            except:\n",
    "                continue\n",
    "        score_list.append({'Symbol': row['symbol'], 'Score': total_score})\n",
    "\n",
    "    score_df = pd.DataFrame(score_list)\n",
    "    if score_df.empty:\n",
    "        out_sheet.range(\"A:E\").clear_contents()\n",
    "        out_sheet.range(\"A1\").options(index=False).value = [[\"Name\", \"Symbol\", \"Score\", \"Signal Range (-10 to 10)\", \"Signal Text\"]]\n",
    "        print(\"No scores computed.\")\n",
    "        return\n",
    "\n",
    "    # --- Normalize and assign signal text ---\n",
    "    score_min = score_df['Score'].min()\n",
    "    score_max = score_df['Score'].max()\n",
    "    norm_vals, signal_texts = [], []\n",
    "    for s in score_df['Score']:\n",
    "        norm, sig = classify_signal(s, score_min, score_max)\n",
    "        norm_vals.append(norm)\n",
    "        signal_texts.append(sig)\n",
    "    score_df['Signal Range (-10 to 10)'] = norm_vals\n",
    "    score_df['Signal Text'] = signal_texts\n",
    "\n",
    "    # --- Add Name column using robust mapping ---\n",
    "    nse_map = get_nse_symbol_map()\n",
    "    bse_map = get_bse_symbol_map()\n",
    "    score_df['Name'] = score_df['Symbol'].apply(lambda s: resolve_name(s, nse_map, bse_map))\n",
    "\n",
    "    # --- Sort by abs value of normalized score, pick top N ---\n",
    "    score_df['abs_score'] = score_df['Signal Range (-10 to 10)'].abs()\n",
    "    topn = score_df.sort_values('abs_score', ascending=False).head(n_picks)\n",
    "    topn = topn[['Name', 'Symbol', 'Score', 'Signal Range (-10 to 10)', 'Signal Text']]\n",
    "\n",
    "    # --- Output to sheet (columns A-E) ---\n",
    "    out_sheet.range(\"A:E\").clear_contents()\n",
    "    out_sheet.range(\"A1\").options(index=False).value = topn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xw.Book(\"rubikview.xlsm\").set_mock_caller()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d75f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported 7134 rows to c:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\Rubik_view\\all_signals.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker=\"rubikview.xlsm\"):\n",
    "    curr = Path.cwd()\n",
    "    for parent in [curr] + list(curr.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    raise FileNotFoundError(f\"Could not find {marker} in any parent directory of {curr}\")\n",
    "\n",
    "ROOT = find_project_root()\n",
    "SIGNALS_DB = ROOT / \"Data\" / \"Signals Data\" / \"signals.duckdb\"\n",
    "\n",
    "def export_all_signals_to_csv(out_csv=\"all_signals.csv\"):\n",
    "    with duckdb.connect(str(SIGNALS_DB), read_only=True) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT *\n",
    "            FROM signals\n",
    "            ORDER BY symbol, date DESC\n",
    "        \"\"\").fetchdf()\n",
    "    out_path = ROOT / out_csv\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… Exported {len(df)} rows to {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "export_all_signals_to_csv(\"all_signals.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
